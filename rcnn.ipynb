{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ead0419",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20fb3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as Fnn\n",
    "import torchvision.transforms.functional as TF\n",
    "#\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "#\n",
    "try:\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    SKL_OK = True\n",
    "except Exception:\n",
    "    SKL_OK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant, global variables\n",
    "DRIVE_PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "ZIP_NAME          = \"MOT16.zip\"\n",
    "TEST_SEQ          = \"MOT16-03\"\n",
    "RUN_ALL_TEST_SEQ  = False\n",
    "DATA_SPLIT_RATIO  = 0.8\n",
    "EPOCHS            = 5\n",
    "BATCH_SIZE        = 2\n",
    "CONF_THRESHOLD    = 0.7\n",
    "IOU_THRESHOLD     = 0.5\n",
    "SEED              = 42\n",
    "#\n",
    "PROJECT_DIR   = f\"{DRIVE_PROJECT_DIR}/Faster RCNN\"\n",
    "ZIP_PATH      = f\"Faster RCNN/MOT_16-Data/{ZIP_NAME}\"\n",
    "DATA_DIR      = \"MOT16\"\n",
    "MODEL_DIR     = f\"{PROJECT_DIR}/models_FasterRCNN\"\n",
    "RESULT_DIR    = f\"{PROJECT_DIR}/results_FasterRCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9029f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constant variables for the object box detection and tracking settings\n",
    "FRAME_COL  = 0\n",
    "ID_COL     = 1\n",
    "LEFT_COL   = 2\n",
    "TOP_COL    = 3\n",
    "WIDTH_COL  = 4\n",
    "HEIGHT_COL = 5\n",
    "PERSON_COL = 6\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "MODEL_PATH  = f\"{MODEL_DIR}/bbox_detector.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3c3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the seed for the project\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79bc78d",
   "metadata": {},
   "source": [
    "# Data Augmentation Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c73f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose class: Chains a sequence of transformations together.\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# ToTensor class: (Placeholder) Applies minimal/no transformation; used for interface compatibility.\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        return image, target\n",
    "\n",
    "# RandomHorizontalFlip class: Randomly horizontally flips the image and its bounding boxes with probability p.\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p and \"boxes\" in target and len(target[\"boxes\"]) > 0:\n",
    "            _, h, w = image.shape\n",
    "            image = TF.hflip(image)\n",
    "            boxes = target[\"boxes\"].clone()\n",
    "            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n",
    "            target[\"boxes\"] = boxes\n",
    "        return image, target\n",
    "\n",
    "# RandomScale class: Randomly resizes the image and scales the bounding boxes accordingly within a range.\n",
    "class RandomScale:\n",
    "    def __init__(self, scale_range=(0.9, 1.1)):\n",
    "        self.scale_range = scale_range\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        _, h, w = image.shape\n",
    "        s = random.uniform(*self.scale_range)\n",
    "        new_h, new_w = int(h*s), int(w*s)\n",
    "        image = TF.resize(image, [new_h, new_w])\n",
    "        if \"boxes\" in target and len(target[\"boxes\"]) > 0:\n",
    "            boxes = target[\"boxes\"].clone()\n",
    "            boxes[:, [0, 2]] *= (new_w / w)\n",
    "            boxes[:, [1, 3]] *= (new_h / h)\n",
    "            target[\"boxes\"] = boxes\n",
    "        return image, target\n",
    "\n",
    "# ColorJitterLite class: Randomly perturbs image brightness, contrast, and saturation.\n",
    "class ColorJitterLite:\n",
    "    def __init__(self, b=0.2, c=0.2, s=0.2):\n",
    "        self.b, self.c, self.s = b, c, s\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < 0.8:\n",
    "            image = TF.adjust_brightness(image, random.uniform(1-self.b, 1+self.b))\n",
    "        if random.random() < 0.8:\n",
    "            image = TF.adjust_contrast(image, random.uniform(1-self.c, 1+self.c))\n",
    "        if random.random() < 0.8:\n",
    "            image = TF.adjust_saturation(image, random.uniform(1-self.s, 1+self.s))\n",
    "        return image, target\n",
    "\n",
    "# RandomGaussianBlur class: Applies Gaussian blur to the image with probability p.\n",
    "class RandomGaussianBlur:\n",
    "    def __init__(self, p=0.25, kernel_size=3, sigma=(0.1, 1.0)):\n",
    "        self.p = p\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            image = TF.gaussian_blur(image, kernel_size=self.kernel_size, sigma=random.uniform(*self.sigma))\n",
    "        return image, target\n",
    "\n",
    "# CutOut class: Randomly masks out square patches from the image with probability p.\n",
    "class CutOut:\n",
    "    def __init__(self, n_holes=2, length=24, p=0.25):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() > self.p:\n",
    "            return image, target\n",
    "        _, h, w = image.shape\n",
    "        img = image.clone()\n",
    "        for _ in range(self.n_holes):\n",
    "            y = random.randint(0, max(0, h - self.length))\n",
    "            x = random.randint(0, max(0, w - self.length))\n",
    "            img[:, y:y+self.length, x:x+self.length] = 0\n",
    "        return img, target\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        ToTensor(),\n",
    "        RandomHorizontalFlip(0.5),\n",
    "        RandomScale((0.9, 1.1)),\n",
    "        ColorJitterLite(0.2, 0.2, 0.2),\n",
    "        RandomGaussianBlur(0.25, 3, (0.1, 1.0)),\n",
    "        CutOut(2, 24, 0.25)\n",
    "    ])\n",
    "\n",
    "def get_test_transforms():\n",
    "    return Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75f318",
   "metadata": {},
   "source": [
    "# Dataset Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18c57605",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jam/Documents/git/Deep Learning/deep-learning-final-project/Faster RCNN/Faster RCNN/MOT_16-Data/MOT16.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZIP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      2\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/zipfile.py:1239\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1241\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jam/Documents/git/Deep Learning/deep-learning-final-project/Faster RCNN/Faster RCNN/MOT_16-Data/MOT16.zip'"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae194d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOT16TrainDataset is used for loading and processing the training data for MOT16 sequences\n",
    "class MOT16TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, seq_list=None, transforms=None):\n",
    "        \"\"\"Initializes the MOT16 training dataset.\n",
    "        Args:\n",
    "            root (str): Path to MOT16 root directory.\n",
    "            seq_list (list, optional): List of sequences to load. Loads all if None.\n",
    "            transforms (callable, optional): Optional transform to be applied.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = []    # List to hold all image relative paths\n",
    "        self.targets = [] # List to hold all target dicts (annotations)\n",
    "\n",
    "        # Get all subdirectories (sequences) in root folder\n",
    "        all_subdirs = sorted(os.listdir(root))\n",
    "        # Use provided sequence list, otherwise use all found sequences\n",
    "        sequence = seq_list if seq_list is not None else all_subdirs\n",
    "\n",
    "        # Iterate over each sequence (video)\n",
    "        for subdir in sequence:\n",
    "            img_dir = os.path.join(root, subdir, \"img1\")         # Directory containing frames\n",
    "            gt_path = os.path.join(root, subdir, \"gt\", \"gt.txt\") # Annotation file path\n",
    "\n",
    "            # Skip if sequence does not contain both img1 folder and gt file\n",
    "            if not (os.path.isdir(img_dir) and os.path.exists(gt_path)): continue\n",
    "\n",
    "            # Get all .jpg frames and sort for chronological order\n",
    "            frames = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")])\n",
    "            # Store relative image paths (subdir/img1/frame.jpg)\n",
    "            self.imgs += [os.path.join(subdir, \"img1\", f) for f in frames]\n",
    "\n",
    "            # Load ground truth annotations for the sequence\n",
    "            gt = np.genfromtxt(gt_path, delimiter=\",\", dtype=float)\n",
    "            # Keep only rows marked as \"person\"\n",
    "            gt = gt[np.where(gt[:, PERSON_COL] == 1)]\n",
    "\n",
    "            # Extract bounding box columns\n",
    "            tops   = gt[:, TOP_COL]\n",
    "            lefts  = gt[:, LEFT_COL]\n",
    "            bots   = tops + gt[:, HEIGHT_COL]\n",
    "            rights = lefts + gt[:, WIDTH_COL]\n",
    "            boxes  = np.column_stack((lefts, tops, rights, bots))\n",
    "\n",
    "            # Extract person ID column for each annotation\n",
    "            person_ids = gt[:, ID_COL]\n",
    "\n",
    "            # For each frame in this sequence\n",
    "            for i in range(len(frames)):\n",
    "                d = {}\n",
    "                # Select the annotations belonging to this frame (frame numbers are 1-based)\n",
    "                mask = (gt[:, FRAME_COL].astype(int) == (i + 1))\n",
    "                req_boxes = boxes[mask, :]\n",
    "                req_ids   = person_ids[mask]\n",
    "                # Store relevant data in dict for this frame\n",
    "                d[\"boxes\"] = torch.tensor(req_boxes, dtype=torch.float32)\n",
    "                d[\"labels\"] = torch.ones(len(req_boxes), dtype=torch.int64)   # Label 1 for 'person' in detection\n",
    "                d[\"person_ids\"] = torch.tensor(req_ids, dtype=torch.int64)    # Identity for each instance\n",
    "                self.targets.append(d)\n",
    "        print(\"[INFO] Train dataset length:\", len(self.imgs))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of images in the dataset.\"\"\"\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Given an index, loads and returns the (image, target) pair.\n",
    "        Args:\n",
    "            idx (int): Index of the image in the dataset.\n",
    "        Returns:\n",
    "            (image, target): Image tensor and target dict, both possibly transformed.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.imgs):\n",
    "            return None, None\n",
    "        # Get relative image path and build absolute path\n",
    "        img_rel = self.imgs[idx]\n",
    "        img_path = os.path.join(self.root, img_rel)\n",
    "        # Load image and normalize to [0, 1] float tensor\n",
    "        img = read_image(img_path).float() / 255.0\n",
    "        # Fetch corresponding annotations\n",
    "        tgt = self.targets[idx]\n",
    "        # Deep copy tensors (avoids accidental mutation)\n",
    "        tgt = {k: (v.clone() if torch.is_tensor(v) else v) for k, v in tgt.items()}\n",
    "        # Apply transforms if provided\n",
    "        if self.transforms:\n",
    "            img, tgt = self.transforms(img, tgt)\n",
    "        return img, tgt\n",
    "\n",
    "# MOT16EvalDataset is used for loading and processing the evaluation (validation) data, \n",
    "# which is similar to training data but does not include identity information.\n",
    "class MOT16EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, seq_list=None, transforms=None):\n",
    "        \"\"\"Initializes the MOT16 evaluation/validation dataset.\n",
    "        Args:\n",
    "            root (str): Path to MOT16 root directory.\n",
    "            seq_list (list, optional): List of sequences to load. Loads all if None.\n",
    "            transforms (callable, optional): Optional transform to be applied.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = []    # List to hold all image relative paths\n",
    "        self.targets = [] # List to hold all target dicts (annotations)\n",
    "        \n",
    "        # Get all subdirectories (sequences) in root folder\n",
    "        all_subdirs = sorted(os.listdir(root))\n",
    "        # Use provided sequence list, otherwise use all found sequences\n",
    "        sequence = seq_list if seq_list is not None else all_subdirs\n",
    "\n",
    "        # Iterate over each sequence (video)\n",
    "        for subdir in sequence:\n",
    "            img_dir = os.path.join(root, subdir, \"img1\")\n",
    "            gt_path = os.path.join(root, subdir, \"gt\", \"gt.txt\")\n",
    "\n",
    "            # Skip if sequence does not contain both img1 folder and gt file\n",
    "            if not (os.path.isdir(img_dir) and os.path.exists(gt_path)):\n",
    "                continue\n",
    "\n",
    "            # Get all .jpg frames and sort for chronological order\n",
    "            frames = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")])\n",
    "            # Store relative image paths (subdir/img1/frame.jpg)\n",
    "            self.imgs += [os.path.join(subdir, \"img1\", f) for f in frames]\n",
    "\n",
    "            # Load ground truth annotations for the sequence\n",
    "            gt = np.genfromtxt(gt_path, delimiter=\",\", dtype=float)\n",
    "            # Keep only rows marked as \"person\"\n",
    "            gt = gt[np.where(gt[:, PERSON_COL] == 1)]\n",
    "\n",
    "            # Extract bounding box columns\n",
    "            tops   = gt[:, TOP_COL]\n",
    "            lefts  = gt[:, LEFT_COL]\n",
    "            bots   = tops + gt[:, HEIGHT_COL]\n",
    "            rights = lefts + gt[:, WIDTH_COL]\n",
    "            boxes  = np.column_stack((lefts, tops, rights, bots))\n",
    "\n",
    "            # For each frame in this sequence\n",
    "            for i in range(len(frames)):\n",
    "                d = {}\n",
    "                # Select the annotations belonging to this frame (frame numbers are 1-based)\n",
    "                mask = (gt[:, FRAME_COL].astype(int) == (i + 1))\n",
    "                req_boxes = boxes[mask, :]\n",
    "                # Store relevant data in dict for this frame\n",
    "                d[\"boxes\"] = torch.tensor(req_boxes, dtype=torch.float32)\n",
    "                d[\"labels\"] = torch.ones(len(req_boxes), dtype=torch.int64)  # Label 1 for 'person'\n",
    "                self.targets.append(d)\n",
    "        print(\"[INFO] Eval dataset length:\", len(self.imgs))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of images in the dataset.\"\"\"\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Given an index, loads and returns the (image, target) pair.\n",
    "        Args:\n",
    "            idx (int): Index of the image in the dataset.\n",
    "        Returns:\n",
    "            (image, target): Image tensor and target dict, both possibly transformed.\n",
    "        \"\"\"\n",
    "        if idx >= len(self.imgs):\n",
    "            return None, None\n",
    "        # Get relative image path and build absolute path\n",
    "        img_rel = self.imgs[idx]\n",
    "        img_path = os.path.join(self.root, img_rel)\n",
    "        # Load image and normalize to [0, 1] float tensor\n",
    "        img = read_image(img_path).float() / 255.0\n",
    "        # Fetch corresponding annotations\n",
    "        tgt = self.targets[idx]\n",
    "        # Deep copy tensors (avoids accidental mutation)\n",
    "        tgt = {k: (v.clone() if torch.is_tensor(v) else v) for k, v in tgt.items()}\n",
    "        # Apply transforms if provided\n",
    "        if self.transforms:\n",
    "            img, tgt = self.transforms(img, tgt)\n",
    "        return img, tgt\n",
    "\n",
    "# MOT16TestDataset is used for loading images from the test set, where no ground-truth annotation is available\n",
    "class MOT16TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        \"\"\"Initializes the MOT16 test dataset (no ground truth available).\n",
    "        Args:\n",
    "            root (str): Path to a single test MOT16 sequence directory.\n",
    "            transforms (callable, optional): Optional transform to be applied.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # List all images under img1/ subfolder, sorted by filename (sequence order)\n",
    "        self.imgs = sorted([f for f in os.listdir(os.path.join(root, \"img1\")) if f.lower().endswith(\".jpg\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of images in the test set.\"\"\"\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Given an index, load and return (image, filename).\n",
    "        Args:\n",
    "            idx (int): Index of the image in the sequence.\n",
    "        Returns:\n",
    "            (image, filename): Image tensor (possibly transformed), and relative filename.\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.root, \"img1\", self.imgs[idx])\n",
    "        # Load image and normalize to [0, 1] float tensor\n",
    "        img = read_image(img_path).float() / 255.0\n",
    "        # Apply transforms if provided (target is dummy empty dict)\n",
    "        if self.transforms:\n",
    "            img, _ = self.transforms(img, {})\n",
    "        return img, self.imgs[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844446b2",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7fe569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_model(weights_path=None):\n",
    "    \"\"\"\n",
    "    Initializes a Faster R-CNN object detector model, optionally loading pretrained weights.\n",
    "    Args:\n",
    "        weights_path (str, optional): Path to the model weights file. If provided and exists,\n",
    "                                      the model will load these weights. Otherwise, uses default weights.\n",
    "    Returns:\n",
    "        model: The initialized (and possibly pretrained) Faster R-CNN detection model.\n",
    "    \"\"\"\n",
    "    # Create a Faster R-CNN model with a ResNet50 backbone, using torchvision's default COCO weights.\n",
    "    model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    # Determine the number of input features for the classifier head (output of the ROI head).\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the prediction head to use the required number of classes for our task.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "    # If a weights file is provided and exists, load these fine-tuned/trained weights into the model.\n",
    "    if weights_path and os.path.exists(weights_path):\n",
    "        state = torch.load(weights_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state)\n",
    "        print(f\"[OK] Loaded weights: {weights_path}\")\n",
    "    else:\n",
    "        # If weights not supplied or file does not exist, warn user (expected on first training run).\n",
    "        print(\"[WARN] Training Weights not loaded (Normal for first time training)\")\n",
    "    # Return the initialized (and possibly pretrained) model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643ef80",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8444f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Filter out any samples where the image is None\n",
    "    batch = [(img,tgt) for img,tgt in batch if img is not None]\n",
    "    # Unzip batch into two lists: list of images and list of targets; handle empty batch\n",
    "    return list(zip(*batch)) if batch else ([], [])\n",
    "\n",
    "def train(model, data_root, seq_list=None, num_epochs=5, batch_size=2, save_path=MODEL_PATH):\n",
    "    # Select device (\"cuda\" if available, else \"cpu\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to device\n",
    "\n",
    "    # Freeze backbone parameters so only head layers will be trained\n",
    "    for p in model.backbone.parameters(): \n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Collect parameters to optimize (those that require gradients)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    # AdamW optimizer with fixed learning rate and weight decay\n",
    "    optimizer = torch.optim.AdamW(params, lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Build training dataset and dataloader with specified transforms and collate_fn\n",
    "    ds = MOT16TrainDataset(os.path.join(data_root, \"train\"), seq_list=seq_list, transforms=get_train_transforms())\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Compute total steps and warmup for LR schedule\n",
    "    total_steps = max(1, len(dl) * num_epochs)\n",
    "    warmup = max(10, int(0.1 * total_steps))\n",
    "    def lr_lambda(step):\n",
    "        # Linear warmup, cosine decay for learning rate\n",
    "        if step < warmup: \n",
    "            return float(step) / float(max(1, warmup))\n",
    "        prog = (step - warmup) / float(max(1, total_steps - warmup))\n",
    "        import math\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * prog))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()   # Set model to training mode\n",
    "        epoch_loss = 0  # Track total loss per epoch\n",
    "        t0 = time.time()  # Start timer\n",
    "\n",
    "        for imgs, tgts in dl:\n",
    "            # Move images to device\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            # Move targets to device (for all tensor values in targets)\n",
    "            tgts = [{k: (v.to(device) if torch.is_tensor(v) else v) for k, v in t.items()} for t in tgts]\n",
    "            # Forward pass: get loss dict from model\n",
    "            loss_dict = model(imgs, tgts)\n",
    "            # Sum all individual loss components to get total loss\n",
    "            loss = sum(loss_dict.values())\n",
    "            # Backpropagation and parameter update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Update LR scheduler\n",
    "            scheduler.step(); step += 1\n",
    "            # Accumulate loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print epoch summary: average loss and elapsed time\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] loss={epoch_loss/len(dl):.4f}  time={time.time()-t0:.1f}s\")\n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    print(f\"[OK] Training Complete → {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f52e",
   "metadata": {},
   "source": [
    "# Metrics & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5239696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = (boxes1[:,2]-boxes1[:,0]).clamp(min=0) * (boxes1[:,3]-boxes1[:,1]).clamp(min=0)\n",
    "    area2 = (boxes2[:,2]-boxes2[:,0]).clamp(min=0) * (boxes2[:,3]-boxes2[:,1]).clamp(min=0)\n",
    "    lt = torch.max(boxes1[:,None,:2], boxes2[:,:2])\n",
    "    rb = torch.min(boxes1[:,None,2:], boxes2[:,2:])\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:,:,0]*wh[:,:,1]\n",
    "    union = area1[:,None] + area2 - inter\n",
    "    return inter / torch.clamp(union, min=1e-6)\n",
    "\n",
    "def compute_metrics(all_scores, all_tp, total_gt, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if len(all_scores)==0:\n",
    "        results={\"mAP\":0.0,\"Total GT boxes\":total_gt,\"Total detections\":0,\"True positives\":0,\"False positives\":0,\n",
    "                 \"Precision_at_conf\":0.0,\"Recall_at_conf\":0.0}\n",
    "        with open(os.path.join(out_dir,\"metrics.txt\"),\"w\") as f:\n",
    "            for k,v in results.items(): f.write(f\"{k}: {v}\\n\")\n",
    "        return results\n",
    "\n",
    "    scores_np = np.array(all_scores)\n",
    "    tps_np    = np.array(all_tp).astype(np.int32)\n",
    "\n",
    "    order = np.argsort(-scores_np)\n",
    "    scores_np = scores_np[order]\n",
    "    tps_np    = tps_np[order]\n",
    "\n",
    "    cum_tp = np.cumsum(tps_np)\n",
    "    cum_fp = np.cumsum(1 - tps_np)\n",
    "\n",
    "    precision = cum_tp / np.maximum(cum_tp + cum_fp, 1)\n",
    "    recall    = cum_tp / max(total_gt, 1)\n",
    "\n",
    "    if SKL_OK:\n",
    "        from sklearn.metrics import average_precision_score\n",
    "        ap = float(average_precision_score(tps_np, scores_np))\n",
    "    else:\n",
    "        ap=0.0\n",
    "        for r in np.linspace(0,1,11):\n",
    "            p = precision[recall>=r].max() if np.any(recall>=r) else 0\n",
    "            ap += p/11.0\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (AP={ap:.4f})\"); plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir,\"precision_recall_curve.png\")); plt.close()\n",
    "\n",
    "    results = {\n",
    "        \"mAP\": ap,\n",
    "        \"Total GT boxes\": int(total_gt),\n",
    "        \"Total detections\": int(len(all_scores)),\n",
    "        \"True positives\": int(cum_tp[-1]),\n",
    "        \"False positives\": int(cum_fp[-1]),\n",
    "        \"Precision_at_conf\": float(precision[-1]),\n",
    "        \"Recall_at_conf\": float(recall[-1]),\n",
    "    }\n",
    "    with open(os.path.join(out_dir,\"metrics.txt\"),\"w\") as f:\n",
    "        for k,v in results.items(): f.write(f\"{k}: {v}\\n\")\n",
    "    print(\"[OK] Writing metrics to:\", os.path.join(out_dir,\"metrics.txt\"))\n",
    "    return results\n",
    "\n",
    "def evaluate_on_folder(model, eval_root, seq_list, out_dir, conf_thr=0.7, iou_thr=0.5, device=None): # Added sequence list\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    ds = MOT16EvalDataset(eval_root, seq_list=seq_list, transforms=get_test_transforms())\n",
    "\n",
    "    all_scores=[]; all_tp=[]; total_gt=0\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    vis_dir = os.path.join(out_dir, \"visualizations\"); os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    for idx in range(len(ds)):\n",
    "        img, tgt = ds[idx]\n",
    "        gt_boxes = tgt[\"boxes\"]\n",
    "        total_gt += len(gt_boxes)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model([img.to(device)])[0]\n",
    "        scores = pred[\"scores\"].detach().cpu()\n",
    "        boxes  = pred[\"boxes\"].detach().cpu()\n",
    "        keep = scores >= conf_thr\n",
    "        scores, boxes = scores[keep], boxes[keep]\n",
    "\n",
    "        tps = []\n",
    "        if len(gt_boxes)>0 and len(boxes)>0:\n",
    "            iou = box_iou(boxes, gt_boxes)\n",
    "            matched_gt = set()\n",
    "            for pi in range(len(boxes)):\n",
    "                gi = torch.argmax(iou[pi]).item()\n",
    "                max_iou = iou[pi, gi].item()\n",
    "                if max_iou >= iou_thr and gi not in matched_gt:\n",
    "                    tps.append(1); matched_gt.add(gi)\n",
    "                else:\n",
    "                    tps.append(0)\n",
    "        elif len(boxes)>0:\n",
    "            tps = [0]*len(boxes)\n",
    "\n",
    "        all_scores += scores.tolist()\n",
    "        all_tp     += tps\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            labels = [f\"{s:.2f}\" for s in scores.tolist()]\n",
    "            vis = draw_bounding_boxes((img*255).to(torch.uint8), boxes, labels=labels, width=2)\n",
    "            write_jpeg(vis, os.path.join(vis_dir, f\"det_{idx:06d}.jpg\"))\n",
    "\n",
    "    return compute_metrics(all_scores, all_tp, total_gt, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17535673",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_TABLE = f\"{RESULT_DIR}/metrics_summary.csv\"\n",
    "\n",
    "def save_metrics_row(results:dict, seq_name:str, model_tag:str, conf_thr:float, notes:str=\"\"):\n",
    "    row = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"seq\": seq_name,\n",
    "        \"model\": model_tag,\n",
    "        \"conf_threshold\": conf_thr,\n",
    "        \"mAP\": results.get(\"mAP\"),\n",
    "        \"total_gt\": results.get(\"Total GT boxes\"),\n",
    "        \"total_det\": results.get(\"Total detections\"),\n",
    "        \"tp\": results.get(\"True positives\"),\n",
    "        \"fp\": results.get(\"False positives\"),\n",
    "        \"precision_at_conf\": results.get(\"Precision_at_conf\"),\n",
    "        \"recall_at_conf\": results.get(\"Recall_at_conf\"),\n",
    "        \"notes\": notes,\n",
    "    }\n",
    "    df = pd.DataFrame([row])\n",
    "    os.makedirs(os.path.dirname(METRICS_TABLE), exist_ok=True)\n",
    "    if os.path.exists(METRICS_TABLE):\n",
    "        df.to_csv(METRICS_TABLE, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(METRICS_TABLE, index=False)\n",
    "    print(f\"[OK] Add metrics to {METRICS_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a0c31",
   "metadata": {},
   "source": [
    "# Video Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8b48224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, txt_output, threshold=0.7):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    ds = MOT16TestDataset(sequence_dir, transforms=get_test_transforms())\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if len(ds)==0:\n",
    "        print(f\"[ERR] {sequence_dir}/img1 is empty\"); return\n",
    "    print(f\"[INFO] Running inference on frames：{len(ds)} frames: {sequence_dir}\")\n",
    "\n",
    "    with open(txt_output, \"w\") as f_out:\n",
    "        for frame_idx in range(len(ds)):\n",
    "            img, name = ds[frame_idx]\n",
    "            with torch.no_grad():\n",
    "                pred = model([img.to(device)])[0]\n",
    "\n",
    "            boxes  = pred[\"boxes\"].detach().cpu()\n",
    "            scores = pred[\"scores\"].detach().cpu()\n",
    "\n",
    "            for box, conf in zip(boxes, scores):\n",
    "                if conf >= threshold:\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w, h = x2-x1, y2-y1\n",
    "                    track_id=0 # placeholder ids because detection only\n",
    "\n",
    "                    f_out.write(f\"{frame_idx+1},{track_id},{int(x1)},{int(y1)},{int(w)},{int(h)},{conf},-1,-1,-1\\n\")\n",
    "    print(f\"[OK] Detections saved to {out_dir}\")\n",
    "\n",
    "def get_sequence_info(sequence_dir):\n",
    "    seqinfo_path = os.path.join(sequence_dir, \"seqinfo.ini\")\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(seqinfo_path)\n",
    "    fps = None\n",
    "    width = None\n",
    "    height = None\n",
    "    fps = config.getint(\"Sequence\", \"frameRate\", fallback=fps)\n",
    "    width = config.getint(\"Sequence\", \"imWidth\", fallback=width)\n",
    "    height = config.getint(\"Sequence\", \"imHeight\", fallback=height)\n",
    "    return fps, (width, height)\n",
    "\n",
    "def visualize_sequence(sequence_dir, results_file, output_video_path):\n",
    "    # Load results\n",
    "    results_df = pd.read_csv(results_file, header=None)\n",
    "    results_df.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"x3\",\"y3\",\"z3\"]\n",
    "\n",
    "    # Load frames\n",
    "    image_paths = sorted(glob.glob(os.path.join(sequence_dir, \"img1\", \"*.jpg\")))\n",
    "\n",
    "    # Video writer\n",
    "    fps, frame_size = get_sequence_info(sequence_dir)\n",
    "    first_img = cv2.imread(image_paths[0])\n",
    "    frame_size = (first_img.shape[1], first_img.shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "\n",
    "    # Assign random colors for IDs\n",
    "    max_id = int(results_df[\"id\"].max()) + 1\n",
    "    np.random.seed(42)  # reproducible colors\n",
    "    colors = np.random.randint(0, 255, size=(max_id, 3), dtype=np.uint8)\n",
    "\n",
    "    # Process frames\n",
    "    for img_path in image_paths:\n",
    "        frame_num = int(os.path.splitext(os.path.basename(img_path))[0])\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        frame_data = results_df[results_df[\"frame\"] == frame_num]\n",
    "        for _, row in frame_data.iterrows():\n",
    "            track_id = int(row[\"id\"])\n",
    "            color = tuple(map(int, colors[track_id]))\n",
    "            x, y, w_box, h_box = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
    "            cv2.rectangle(img, (x, y), (x + w_box, y + h_box), color, 2)\n",
    "            cv2.putText(img, str(track_id), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        video_writer.write(img)\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf354b3",
   "metadata": {},
   "source": [
    "# MAIN CODE EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c254764",
   "metadata": {},
   "source": [
    "## Train | Validation | Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150190b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_split(folders_dir, split_ratio, seed):\n",
    "    sequences = [d for d in os.listdir(folders_dir) if os.path.isdir(os.path.join(folders_dir, d))]\n",
    "    sequences.sort()\n",
    "    print(\"All sequences:\", sequences)\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(sequences)\n",
    "\n",
    "    split_idx = int(len(sequences) * split_ratio)\n",
    "    train_sequence = sequences[:split_idx]\n",
    "    eval_sequence = sequences[split_idx:]\n",
    "\n",
    "    print(\"Training sequences:\", train_sequence)\n",
    "    print(\"Evaluation sequences:\", eval_sequence)\n",
    "\n",
    "    return train_sequence, eval_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b3df7",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d8bac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MOT16/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_sequence, eval_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolders_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_SPLIT_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Split Data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m get_detector_model(weights_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m train(model, DATA_DIR, seq_list\u001b[38;5;241m=\u001b[39mtrain_sequence, num_epochs\u001b[38;5;241m=\u001b[39mEPOCHS, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, save_path\u001b[38;5;241m=\u001b[39mMODEL_PATH)\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mtrain_eval_split\u001b[0;34m(folders_dir, split_ratio, seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_eval_split\u001b[39m(folders_dir, split_ratio, seed):\n\u001b[0;32m----> 2\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolders_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folders_dir, d))]\n\u001b[1;32m      3\u001b[0m     sequences\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll sequences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequences)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MOT16/train'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_sequence, eval_sequence = train_eval_split(folders_dir=os.path.join(DATA_DIR, \"train\"), split_ratio=DATA_SPLIT_RATIO, seed=SEED) # Split Data\n",
    "\n",
    "model = get_detector_model(weights_path=None)\n",
    "train(model, DATA_DIR, seq_list=train_sequence, num_epochs=EPOCHS, batch_size=BATCH_SIZE, save_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a031018",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_detector_model(weights_path=MODEL_PATH)\n",
    "\n",
    "eval_root = os.path.join(DATA_DIR, \"train\")\n",
    "eval_out  = os.path.join(RESULT_DIR, f\"eval_train_conf{CONF_THRESHOLD}\")\n",
    "res = evaluate_on_folder(model, eval_root, seq_list=eval_sequence, out_dir=eval_out, conf_thr=CONF_THRESHOLD, iou_thr=IOU_THRESHOLD, device=device)\n",
    "save_metrics_row(res, seq_name=\"TRAIN_ALL\", model_tag=\"fasterrcnn_head_finetune\", conf_thr=CONF_THRESHOLD, notes=\"aug+AdamW+warmup_cos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1f15a",
   "metadata": {},
   "source": [
    "## Test Video Creation to Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_detector_model(weights_path=MODEL_PATH)\n",
    "\n",
    "# e) Single sequence or complete sequence (testing = no ground truth)\n",
    "test_seqs = [\"MOT16-01\",\"MOT16-03\",\"MOT16-06\",\"MOT16-07\",\"MOT16-08\",\"MOT16-12\",\"MOT16-14\"]\n",
    "if RUN_ALL_TEST_SEQ:\n",
    "    for seq in test_seqs:\n",
    "        sequence_dir = os.path.join(DATA_DIR, \"test\", seq)\n",
    "        out_dir = os.path.join(RESULT_DIR, f\"test_{seq}\")\n",
    "        text_output = os.path.join(out_dir, f\"test_{seq}.txt\")\n",
    "        fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
    "        video_path = os.path.join(out_dir, f\"test_{seq}.mp4\")\n",
    "        visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
    "else:\n",
    "    seq = TEST_SEQ\n",
    "    sequence_dir = os.path.join(DATA_DIR, \"test\", seq)\n",
    "    out_dir = os.path.join(RESULT_DIR, f\"test_{seq}\")\n",
    "    text_output = os.path.join(out_dir, f\"test_{seq}.txt\")\n",
    "    fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
    "    video_path = os.path.join(out_dir, f\"test_{seq}.mp4\")\n",
    "    visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
    "\n",
    "print(\"\\n[DONE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d968714",
   "metadata": {},
   "source": [
    "# Training Video Creation Based on Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4db2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_detector_model(weights_path=MODEL_PATH)\n",
    "\n",
    "# e) Single sequence or complete sequence (testing = no ground truth)\n",
    "test_seqs = [\"MOT16-01\",\"MOT16-03\",\"MOT16-06\",\"MOT16-07\",\"MOT16-08\",\"MOT16-12\",\"MOT16-14\"]\n",
    "if RUN_ALL_TEST_SEQ:\n",
    "    for seq in test_seqs:\n",
    "        sequence_dir = os.path.join(DATA_DIR, \"test\", seq)\n",
    "        out_dir = os.path.join(RESULT_DIR, f\"test_{seq}\")\n",
    "        text_output = os.path.join(out_dir, f\"test_{seq}.txt\")\n",
    "        fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
    "        video_path = os.path.join(out_dir, f\"test_{seq}.mp4\")\n",
    "        visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
    "else:\n",
    "    seq = TEST_SEQ\n",
    "    sequence_dir = os.path.join(DATA_DIR, \"test\", seq)\n",
    "    out_dir = os.path.join(RESULT_DIR, f\"test_{seq}\")\n",
    "    text_output = os.path.join(out_dir, f\"test_{seq}.txt\")\n",
    "    fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
    "    video_path = os.path.join(out_dir, f\"test_{seq}.mp4\")\n",
    "    visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
    "\n",
    "print(\"\\n[DONE]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
