{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP-SCI 5567 — Faster R-CNN Pipeline (Project)\n",
        "Ailing Nan | Jim Huynh | Joseph Marinello | Kenny Phan"
      ],
      "metadata": {
        "id": "JhxWb7ukCygW"
      },
      "id": "JhxWb7ukCygW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount users Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5OzGNHamv1m",
        "outputId": "a436de3f-08cc-4257-886b-628345cd12bb"
      },
      "id": "S5OzGNHamv1m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All Imports\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import configparser\n",
        "#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Fnn\n",
        "import torchvision.transforms.functional as TF\n",
        "#\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.io import read_image, write_jpeg\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "#\n",
        "try:\n",
        "    from sklearn.metrics import average_precision_score\n",
        "    SKL_OK = True\n",
        "except Exception:\n",
        "    SKL_OK = False\n",
        "\n"
      ],
      "metadata": {
        "id": "dnwPtilGm8Xp"
      },
      "id": "dnwPtilGm8Xp",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9930db9a",
      "metadata": {
        "id": "9930db9a"
      },
      "outputs": [],
      "source": [
        "# Configure directories and constants\n",
        "DRIVE_PROJECT_DIR = \"/content/drive/MyDrive/Deep_Learning/Final Project\" #Please update this based on you Drive Folder Configuration!\n",
        "PROJECT_DIR       = DRIVE_PROJECT_DIR\n",
        "ZIP_NAME          = \"MOT16.zip\" #Please make sure the zip file is located at the root folder!\n",
        "ZIP_PATH          = f\"{PROJECT_DIR}/{ZIP_NAME}\"\n",
        "MODEL_DIR         = f\"{PROJECT_DIR}/models_FasterRCNN\"\n",
        "RESULT_DIR        = f\"{PROJECT_DIR}/results_FasterRCNN\"\n",
        "DATA_DIR          = f\"{PROJECT_DIR}/MOT16\"\n",
        "TEST_SEQ          = \"MOT16-03\"\n",
        "#\n",
        "RUN_ALL_TEST_SEQ  = False\n",
        "DATA_SPLIT_RATIO  = 0.8\n",
        "EPOCHS            = 5\n",
        "BATCH_SIZE        = 2\n",
        "CONF_THRESHOLD    = 0.7\n",
        "IOU_THRESHOLD     = 0.5\n",
        "SEED              = 42\n",
        "\n",
        "# Set all seeds\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "bb6c34b6",
      "metadata": {
        "id": "bb6c34b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ab3615-2d81-4387-e03f-24d0bb567d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Skip unzip：Data directory already exist\n"
          ]
        }
      ],
      "source": [
        "# Ensure directory folders are created\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Ensure data can be found within directories\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    assert os.path.exists(ZIP_PATH), f\"[ERR] {ZIP_PATH} not found, please upload {ZIP_NAME} to {PROJECT_DIR}/\"\n",
        "    print(\"[INFO] Unzip MOT16.zip ...\")\n",
        "    !unzip -q \"{ZIP_PATH}\" -d \"{PROJECT_DIR}\"\n",
        "    print(\"[OK] File has been Unzipped\")\n",
        "else:\n",
        "    print(\"[INFO] Skip unzip：Data directory already exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b8fa6023",
      "metadata": {
        "id": "b8fa6023"
      },
      "outputs": [],
      "source": [
        "# Frame Constants\n",
        "FRAME_COL  = 0\n",
        "ID_COL     = 1\n",
        "LEFT_COL   = 2\n",
        "TOP_COL    = 3\n",
        "WIDTH_COL  = 4\n",
        "HEIGHT_COL = 5\n",
        "PERSON_COL = 6\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "MODEL_PATH  = f\"{MODEL_DIR}/bbox_detector.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "af32cc12",
      "metadata": {
        "id": "af32cc12"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation Classes\n",
        "class Compose:\n",
        "    def __init__(self, transforms): self.transforms = transforms\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        return image, target\n",
        "\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, p=0.5): self.p=p\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.p and \"boxes\" in target and len(target[\"boxes\"])>0:\n",
        "            _, h, w = image.shape\n",
        "            image = TF.hflip(image)\n",
        "            boxes = target[\"boxes\"].clone()\n",
        "            boxes[:, [0,2]] = w - boxes[:, [2,0]]\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "class RandomScale:\n",
        "    def __init__(self, scale_range=(0.9, 1.1)): self.scale_range = scale_range\n",
        "    def __call__(self, image, target):\n",
        "        _, h, w = image.shape\n",
        "        s = random.uniform(*self.scale_range)\n",
        "        new_h, new_w = int(h*s), int(w*s)\n",
        "        image = TF.resize(image, [new_h, new_w])\n",
        "        if \"boxes\" in target and len(target[\"boxes\"])>0:\n",
        "            boxes = target[\"boxes\"].clone()\n",
        "            boxes[:, [0,2]] *= (new_w / w)\n",
        "            boxes[:, [1,3]] *= (new_h / h)\n",
        "            target[\"boxes\"] = boxes\n",
        "        return image, target\n",
        "\n",
        "class ColorJitterLite:\n",
        "    def __init__(self, b=0.2, c=0.2, s=0.2):\n",
        "        self.b, self.c, self.s = b, c, s\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < 0.8:\n",
        "            image = TF.adjust_brightness(image, random.uniform(1-self.b, 1+self.b))\n",
        "        if random.random() < 0.8:\n",
        "            image = TF.adjust_contrast(image, random.uniform(1-self.c, 1+self.c))\n",
        "        if random.random() < 0.8:\n",
        "            image = TF.adjust_saturation(image, random.uniform(1-self.s, 1+self.s))\n",
        "        return image, target\n",
        "\n",
        "class RandomGaussianBlur:\n",
        "    def __init__(self, p=0.25, kernel_size=3, sigma=(0.1, 1.0)):\n",
        "        self.p = p; self.kernel_size = kernel_size; self.sigma = sigma\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.p:\n",
        "            image = TF.gaussian_blur(image, kernel_size=self.kernel_size, sigma=random.uniform(*self.sigma))\n",
        "        return image, target\n",
        "\n",
        "class CutOut:\n",
        "    def __init__(self, n_holes=2, length=24, p=0.25):\n",
        "        self.n_holes=n_holes; self.length=length; self.p=p\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() > self.p: return image, target\n",
        "        _, h, w = image.shape\n",
        "        img = image.clone()\n",
        "        for _ in range(self.n_holes):\n",
        "            y = random.randint(0, max(0, h - self.length))\n",
        "            x = random.randint(0, max(0, w - self.length))\n",
        "            img[:, y:y+self.length, x:x+self.length] = 0\n",
        "        return img, target\n",
        "\n",
        "def get_train_transforms():\n",
        "    return Compose([ToTensor(), RandomHorizontalFlip(0.5), RandomScale((0.9, 1.1)),\n",
        "                    ColorJitterLite(0.2,0.2,0.2), RandomGaussianBlur(0.25,3,(0.1,1.0)), CutOut(2,24,0.25)])\n",
        "\n",
        "def get_test_transforms():\n",
        "    return Compose([ToTensor()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a0d2384c",
      "metadata": {
        "id": "a0d2384c"
      },
      "outputs": [],
      "source": [
        "# Dataset Handling Classes\n",
        "class MOT16TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, seq_list=None, transforms=None):\n",
        "        self.root = root; self.transforms = transforms\n",
        "        self.imgs=[]; self.targets=[]\n",
        "\n",
        "        # Added sequence or all training sets\n",
        "        all_subdirs = sorted(os.listdir(root))\n",
        "        sequence = seq_list if seq_list is not None else all_subdirs\n",
        "\n",
        "        for subdir in sequence:\n",
        "            img_dir = os.path.join(root, subdir, \"img1\")\n",
        "            gt_path = os.path.join(root, subdir, \"gt\", \"gt.txt\")\n",
        "            if not (os.path.isdir(img_dir) and os.path.exists(gt_path)): continue\n",
        "            frames = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")])\n",
        "            self.imgs += [os.path.join(subdir,\"img1\",f) for f in frames]\n",
        "\n",
        "            gt = np.genfromtxt(gt_path, delimiter=\",\", dtype=float)\n",
        "            gt = gt[np.where(gt[:, PERSON_COL] == 1)]\n",
        "\n",
        "            tops   = gt[:, TOP_COL]\n",
        "            lefts  = gt[:, LEFT_COL]\n",
        "            bots   = tops + gt[:, HEIGHT_COL]\n",
        "            rights = lefts + gt[:, WIDTH_COL]\n",
        "            boxes  = np.column_stack((lefts, tops, rights, bots))\n",
        "            person_ids = gt[:, ID_COL]\n",
        "\n",
        "            for i in range(len(frames)):\n",
        "                d={}\n",
        "                mask = (gt[:, FRAME_COL].astype(int) == (i+1))\n",
        "                req_boxes = boxes[mask,:]\n",
        "                req_ids   = person_ids[mask]\n",
        "                d[\"boxes\"] = torch.tensor(req_boxes, dtype=torch.float32)\n",
        "                d[\"labels\"] = torch.ones(len(req_boxes), dtype=torch.int64)\n",
        "                d[\"person_ids\"] = torch.tensor(req_ids, dtype=torch.int64)\n",
        "                self.targets.append(d)\n",
        "        print(\"[INFO] Train dataset length:\", len(self.imgs))\n",
        "\n",
        "    def __len__(self): return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx>=len(self.imgs): return None,None\n",
        "        img_rel = self.imgs[idx]\n",
        "        img_path = os.path.join(self.root, img_rel)\n",
        "        img = read_image(img_path).float() / 255.0\n",
        "        tgt = self.targets[idx]\n",
        "        tgt = {k:(v.clone() if torch.is_tensor(v) else v) for k,v in tgt.items()}\n",
        "        if self.transforms: img, tgt = self.transforms(img, tgt)\n",
        "        return img, tgt\n",
        "\n",
        "class MOT16EvalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, seq_list=None, transforms=None):\n",
        "        self.root=root; self.transforms=transforms\n",
        "        self.imgs=[]; self.targets=[]\n",
        "\n",
        "        # Added sequence or all training sets\n",
        "        all_subdirs = sorted(os.listdir(root))\n",
        "        sequence = seq_list if seq_list is not None else all_subdirs\n",
        "\n",
        "        for subdir in sequence:\n",
        "            img_dir = os.path.join(root, subdir, \"img1\")\n",
        "            gt_path = os.path.join(root, subdir, \"gt\", \"gt.txt\")\n",
        "            if not (os.path.isdir(img_dir) and os.path.exists(gt_path)): continue\n",
        "            frames = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(\".jpg\")])\n",
        "            self.imgs += [os.path.join(subdir,\"img1\",f) for f in frames]\n",
        "\n",
        "            gt = np.genfromtxt(gt_path, delimiter=\",\", dtype=float)\n",
        "            gt = gt[np.where(gt[:, PERSON_COL] == 1)]\n",
        "\n",
        "            tops   = gt[:, TOP_COL]\n",
        "            lefts  = gt[:, LEFT_COL]\n",
        "            bots   = tops + gt[:, HEIGHT_COL]\n",
        "            rights = lefts + gt[:, WIDTH_COL]\n",
        "            boxes  = np.column_stack((lefts, tops, rights, bots))\n",
        "\n",
        "            for i in range(len(frames)):\n",
        "                d={}\n",
        "                mask = (gt[:, FRAME_COL].astype(int) == (i+1))\n",
        "                req_boxes = boxes[mask,:]\n",
        "                d[\"boxes\"] = torch.tensor(req_boxes, dtype=torch.float32)\n",
        "                d[\"labels\"] = torch.ones(len(req_boxes), dtype=torch.int64)\n",
        "                self.targets.append(d)\n",
        "        print(\"[INFO] Eval dataset length:\", len(self.imgs))\n",
        "\n",
        "    def __len__(self): return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx>=len(self.imgs): return None,None\n",
        "        img_rel = self.imgs[idx]\n",
        "        img_path = os.path.join(self.root, img_rel)\n",
        "        img = read_image(img_path).float() / 255.0\n",
        "        tgt = self.targets[idx]\n",
        "        tgt = {k:(v.clone() if torch.is_tensor(v) else v) for k,v in tgt.items()}\n",
        "        if self.transforms: img, tgt = self.transforms(img, tgt)\n",
        "        return img, tgt\n",
        "\n",
        "class MOT16TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root=root; self.transforms=transforms\n",
        "        self.imgs = sorted([f for f in os.listdir(os.path.join(root,\"img1\")) if f.lower().endswith(\".jpg\")])\n",
        "    def __len__(self): return len(self.imgs)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root,\"img1\", self.imgs[idx])\n",
        "        img = read_image(img_path).float()/255.0\n",
        "        if self.transforms: img,_ = self.transforms(img,{})\n",
        "        return img, self.imgs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4376d3df",
      "metadata": {
        "id": "4376d3df"
      },
      "outputs": [],
      "source": [
        "# Model Creation Functions\n",
        "def get_detector_model(weights_path=None):\n",
        "    model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
        "    if weights_path and os.path.exists(weights_path):\n",
        "        state = torch.load(weights_path, map_location=\"cpu\")\n",
        "        model.load_state_dict(state)\n",
        "        print(f\"[OK] Loaded weights: {weights_path}\")\n",
        "    else:\n",
        "        print(\"[WARN] Training Weights not loaded（Normal for first time training）\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ecd35d86",
      "metadata": {
        "id": "ecd35d86"
      },
      "outputs": [],
      "source": [
        "# Training Functions\n",
        "def collate_fn(batch):\n",
        "    batch = [(img,tgt) for img,tgt in batch if img is not None]\n",
        "    return list(zip(*batch)) if batch else ([], [])\n",
        "\n",
        "def train(model, data_root, seq_list=None, num_epochs=5, batch_size=2, save_path=MODEL_PATH): # added sequence list\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    for p in model.backbone.parameters(): p.requires_grad=False\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(params, lr=5e-4, weight_decay=1e-4)\n",
        "\n",
        "    ds = MOT16TrainDataset(os.path.join(data_root,\"train\"), seq_list=seq_list, transforms=get_train_transforms()) # added sequence list\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "    total_steps = max(1, len(dl)*num_epochs)\n",
        "    warmup = max(10, int(0.1*total_steps))\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup: return float(step) / float(max(1,warmup))\n",
        "        prog = (step-warmup)/float(max(1,total_steps-warmup))\n",
        "        import math\n",
        "        return 0.5*(1.0+math.cos(math.pi*prog))\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    step=0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train(); epoch_loss=0; t0=time.time()\n",
        "        for imgs, tgts in dl:\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            tgts = [{k:(v.to(device) if torch.is_tensor(v) else v) for k,v in t.items()} for t in tgts]\n",
        "            loss_dict = model(imgs, tgts)\n",
        "            loss = sum(loss_dict.values())\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            scheduler.step(); step+=1\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"[Epoch {epoch+1}/{num_epochs}] loss={epoch_loss/len(dl):.4f}  time={time.time()-t0:.1f}s\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    print(f\"[OK] Training Complete → {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d9d8634f",
      "metadata": {
        "id": "d9d8634f"
      },
      "outputs": [],
      "source": [
        "# Metric and Evaluation Functions\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = (boxes1[:,2]-boxes1[:,0]).clamp(min=0) * (boxes1[:,3]-boxes1[:,1]).clamp(min=0)\n",
        "    area2 = (boxes2[:,2]-boxes2[:,0]).clamp(min=0) * (boxes2[:,3]-boxes2[:,1]).clamp(min=0)\n",
        "    lt = torch.max(boxes1[:,None,:2], boxes2[:,:2])\n",
        "    rb = torch.min(boxes1[:,None,2:], boxes2[:,2:])\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    inter = wh[:,:,0]*wh[:,:,1]\n",
        "    union = area1[:,None] + area2 - inter\n",
        "    return inter / torch.clamp(union, min=1e-6)\n",
        "\n",
        "def compute_metrics(all_scores, all_tp, total_gt, out_dir):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if len(all_scores)==0:\n",
        "        results={\"mAP\":0.0,\"Total GT boxes\":total_gt,\"Total detections\":0,\"True positives\":0,\"False positives\":0,\n",
        "                 \"Precision_at_conf\":0.0,\"Recall_at_conf\":0.0}\n",
        "        with open(os.path.join(out_dir,\"metrics.txt\"),\"w\") as f:\n",
        "            for k,v in results.items(): f.write(f\"{k}: {v}\\n\")\n",
        "        return results\n",
        "\n",
        "    scores_np = np.array(all_scores)\n",
        "    tps_np    = np.array(all_tp).astype(np.int32)\n",
        "\n",
        "    order = np.argsort(-scores_np)\n",
        "    scores_np = scores_np[order]\n",
        "    tps_np    = tps_np[order]\n",
        "\n",
        "    cum_tp = np.cumsum(tps_np)\n",
        "    cum_fp = np.cumsum(1 - tps_np)\n",
        "\n",
        "    precision = cum_tp / np.maximum(cum_tp + cum_fp, 1)\n",
        "    recall    = cum_tp / max(total_gt, 1)\n",
        "\n",
        "    if SKL_OK:\n",
        "        from sklearn.metrics import average_precision_score\n",
        "        ap = float(average_precision_score(tps_np, scores_np))\n",
        "    else:\n",
        "        ap=0.0\n",
        "        for r in np.linspace(0,1,11):\n",
        "            p = precision[recall>=r].max() if np.any(recall>=r) else 0\n",
        "            ap += p/11.0\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(recall, precision)\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR (AP={ap:.4f})\"); plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir,\"precision_recall_curve.png\")); plt.close()\n",
        "\n",
        "    results = {\n",
        "        \"mAP\": ap,\n",
        "        \"Total GT boxes\": int(total_gt),\n",
        "        \"Total detections\": int(len(all_scores)),\n",
        "        \"True positives\": int(cum_tp[-1]),\n",
        "        \"False positives\": int(cum_fp[-1]),\n",
        "        \"Precision_at_conf\": float(precision[-1]),\n",
        "        \"Recall_at_conf\": float(recall[-1]),\n",
        "    }\n",
        "    with open(os.path.join(out_dir,\"metrics.txt\"),\"w\") as f:\n",
        "        for k,v in results.items(): f.write(f\"{k}: {v}\\n\")\n",
        "    print(\"[OK] Writing metrics to:\", os.path.join(out_dir,\"metrics.txt\"))\n",
        "    return results\n",
        "\n",
        "def evaluate_on_folder(model, eval_root, seq_list, out_dir, conf_thr=0.7, iou_thr=0.5, device=None): # Added sequence list\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    ds = MOT16EvalDataset(eval_root, seq_list=seq_list, transforms=get_test_transforms())\n",
        "\n",
        "    all_scores=[]; all_tp=[]; total_gt=0\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    vis_dir = os.path.join(out_dir, \"visualizations\"); os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "    for idx in range(len(ds)):\n",
        "        img, tgt = ds[idx]\n",
        "        gt_boxes = tgt[\"boxes\"]\n",
        "        total_gt += len(gt_boxes)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model([img.to(device)])[0]\n",
        "        scores = pred[\"scores\"].detach().cpu()\n",
        "        boxes  = pred[\"boxes\"].detach().cpu()\n",
        "        keep = scores >= conf_thr\n",
        "        scores, boxes = scores[keep], boxes[keep]\n",
        "\n",
        "        tps = []\n",
        "        if len(gt_boxes)>0 and len(boxes)>0:\n",
        "            iou = box_iou(boxes, gt_boxes)\n",
        "            matched_gt = set()\n",
        "            for pi in range(len(boxes)):\n",
        "                gi = torch.argmax(iou[pi]).item()\n",
        "                max_iou = iou[pi, gi].item()\n",
        "                if max_iou >= iou_thr and gi not in matched_gt:\n",
        "                    tps.append(1); matched_gt.add(gi)\n",
        "                else:\n",
        "                    tps.append(0)\n",
        "        elif len(boxes)>0:\n",
        "            tps = [0]*len(boxes)\n",
        "\n",
        "        all_scores += scores.tolist()\n",
        "        all_tp     += tps\n",
        "\n",
        "        if idx % 50 == 0:\n",
        "            labels = [f\"{s:.2f}\" for s in scores.tolist()]\n",
        "            vis = draw_bounding_boxes((img*255).to(torch.uint8), boxes, labels=labels, width=2)\n",
        "            write_jpeg(vis, os.path.join(vis_dir, f\"det_{idx:06d}.jpg\"))\n",
        "\n",
        "    return compute_metrics(all_scores, all_tp, total_gt, out_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d90b6b9b",
      "metadata": {
        "id": "d90b6b9b"
      },
      "outputs": [],
      "source": [
        "# Metric Extraction\n",
        "METRICS_TABLE = f\"{RESULT_DIR}/metrics_summary.csv\"\n",
        "\n",
        "def save_metrics_row(results:dict, seq_name:str, model_tag:str, conf_thr:float, notes:str=\"\"):\n",
        "    row = {\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"seq\": seq_name,\n",
        "        \"model\": model_tag,\n",
        "        \"conf_threshold\": conf_thr,\n",
        "        \"mAP\": results.get(\"mAP\"),\n",
        "        \"total_gt\": results.get(\"Total GT boxes\"),\n",
        "        \"total_det\": results.get(\"Total detections\"),\n",
        "        \"tp\": results.get(\"True positives\"),\n",
        "        \"fp\": results.get(\"False positives\"),\n",
        "        \"precision_at_conf\": results.get(\"Precision_at_conf\"),\n",
        "        \"recall_at_conf\": results.get(\"Recall_at_conf\"),\n",
        "        \"notes\": notes,\n",
        "    }\n",
        "    df = pd.DataFrame([row])\n",
        "    os.makedirs(os.path.dirname(METRICS_TABLE), exist_ok=True)\n",
        "    if os.path.exists(METRICS_TABLE):\n",
        "        df.to_csv(METRICS_TABLE, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(METRICS_TABLE, index=False)\n",
        "    print(f\"[OK] Add metrics to {METRICS_TABLE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c62ec35a",
      "metadata": {
        "id": "c62ec35a"
      },
      "outputs": [],
      "source": [
        "# Inference and Video Generation Functions\n",
        "def fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, txt_output, threshold=0.7):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    ds = MOT16TestDataset(sequence_dir, transforms=get_test_transforms())\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if len(ds)==0:\n",
        "        print(f\"[ERR] {sequence_dir}/img1 is empty\"); return\n",
        "    print(f\"[INFO] Running inference on frames：{len(ds)} frames: {sequence_dir}\")\n",
        "\n",
        "    with open(txt_output, \"w\") as f_out:\n",
        "        for frame_idx in range(len(ds)):\n",
        "            img, name = ds[frame_idx]\n",
        "            with torch.no_grad():\n",
        "                pred = model([img.to(device)])[0]\n",
        "\n",
        "            boxes  = pred[\"boxes\"].detach().cpu()\n",
        "            scores = pred[\"scores\"].detach().cpu()\n",
        "\n",
        "            for box, conf in zip(boxes, scores):\n",
        "                if conf >= threshold:\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    w, h = x2-x1, y2-y1\n",
        "                    track_id=0 # placeholder ids because detection only\n",
        "\n",
        "                    f_out.write(f\"{frame_idx+1},{track_id},{int(x1)},{int(y1)},{int(w)},{int(h)},{conf},-1,-1,-1\\n\")\n",
        "    print(f\"[OK] Detections saved to {out_dir}\")\n",
        "\n",
        "def get_sequence_info(sequence_dir):\n",
        "    seqinfo_path = os.path.join(sequence_dir, \"seqinfo.ini\")\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(seqinfo_path)\n",
        "    fps = None\n",
        "    width = None\n",
        "    height = None\n",
        "    fps = config.getint(\"Sequence\", \"frameRate\", fallback=fps)\n",
        "    width = config.getint(\"Sequence\", \"imWidth\", fallback=width)\n",
        "    height = config.getint(\"Sequence\", \"imHeight\", fallback=height)\n",
        "    return fps, (width, height)\n",
        "\n",
        "def visualize_sequence(sequence_dir, results_file, output_video_path):\n",
        "    # Load results\n",
        "    results_df = pd.read_csv(results_file, header=None)\n",
        "    results_df.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"x3\",\"y3\",\"z3\"]\n",
        "\n",
        "    # Load frames\n",
        "    image_paths = sorted(glob.glob(os.path.join(sequence_dir, \"img1\", \"*.jpg\")))\n",
        "\n",
        "    # Video writer\n",
        "    fps, frame_size = get_sequence_info(sequence_dir)\n",
        "    first_img = cv2.imread(image_paths[0])\n",
        "    frame_size = (first_img.shape[1], first_img.shape[0])\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
        "\n",
        "    # Assign random colors for IDs\n",
        "    max_id = int(results_df[\"id\"].max()) + 1\n",
        "    np.random.seed(42)  # reproducible colors\n",
        "    colors = np.random.randint(0, 255, size=(max_id, 3), dtype=np.uint8)\n",
        "\n",
        "    # Process frames\n",
        "    for img_path in image_paths:\n",
        "        frame_num = int(os.path.splitext(os.path.basename(img_path))[0])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        frame_data = results_df[results_df[\"frame\"] == frame_num]\n",
        "        for _, row in frame_data.iterrows():\n",
        "            track_id = int(row[\"id\"])\n",
        "            color = tuple(map(int, colors[track_id]))\n",
        "            x, y, w_box, h_box = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
        "            cv2.rectangle(img, (x, y), (x + w_box, y + h_box), color, 2)\n",
        "            cv2.putText(img, str(track_id), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        video_writer.write(img)\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {output_video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0b96136b",
      "metadata": {
        "id": "0b96136b"
      },
      "outputs": [],
      "source": [
        "# Split Data Functions\n",
        "def train_eval_split(folders_dir, split_ratio, seed):\n",
        "    sequences = [d for d in os.listdir(folders_dir) if os.path.isdir(os.path.join(folders_dir, d))]\n",
        "    sequences.sort()\n",
        "    print(\"All sequences:\", sequences)\n",
        "\n",
        "    random.seed(seed)\n",
        "    random.shuffle(sequences)\n",
        "\n",
        "    split_idx = int(len(sequences) * split_ratio)\n",
        "    train_sequence = sequences[:split_idx]\n",
        "    eval_sequence = sequences[split_idx:]\n",
        "\n",
        "    print(\"Training sequences:\", train_sequence)\n",
        "    print(\"Evaluation sequences:\", eval_sequence)\n",
        "\n",
        "    return train_sequence, eval_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN CODE EXECUTION"
      ],
      "metadata": {
        "id": "GrJgUNYlratY"
      },
      "id": "GrJgUNYlratY"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e1c177b3",
      "metadata": {
        "id": "e1c177b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7589a2a9-d340-4084-d7a5-5053c9ee5375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All sequences: ['MOT16-02', 'MOT16-04', 'MOT16-05', 'MOT16-09', 'MOT16-10', 'MOT16-11', 'MOT16-13']\n",
            "Training sequences: ['MOT16-04', 'MOT16-09', 'MOT16-10', 'MOT16-05', 'MOT16-13']\n",
            "Evaluation sequences: ['MOT16-02', 'MOT16-11']\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 220MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] Training Weights not loaded（Normal for first time training）\n",
            "[INFO] Train dataset length: 3816\n",
            "[Epoch 1/5] loss=0.6020  time=380.4s\n",
            "[Epoch 2/5] loss=0.4829  time=377.6s\n",
            "[Epoch 3/5] loss=0.4279  time=385.3s\n",
            "[Epoch 4/5] loss=0.3837  time=401.3s\n",
            "[Epoch 5/5] loss=0.3549  time=383.6s\n",
            "[OK] Training Complete → /content/drive/MyDrive/Deep_Learning/Final Project/models_FasterRCNN/bbox_detector.pth\n"
          ]
        }
      ],
      "source": [
        "# TRAINING\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_sequence, eval_sequence = train_eval_split(folders_dir=f\"{DATA_DIR}/train\", split_ratio=DATA_SPLIT_RATIO, seed=SEED) # Split Data\n",
        "\n",
        "# a) Training (If you already have weights and don\"t want to retrain, you can comment out the next 2 lines)\n",
        "model = get_detector_model(weights_path=None)\n",
        "train(model, DATA_DIR, seq_list=train_sequence, num_epochs=EPOCHS, batch_size=BATCH_SIZE, save_path=MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "# b) Reload weights, perform a clean evaluation\n",
        "model = get_detector_model(weights_path=MODEL_PATH)\n",
        "\n",
        "# c) Evaluation (using data with GT)\n",
        "eval_root = os.path.join(DATA_DIR, \"train\")\n",
        "eval_out  = os.path.join(RESULT_DIR, f\"eval_train_conf{CONF_THRESHOLD}\")\n",
        "res = evaluate_on_folder(model, eval_root, seq_list=eval_sequence, out_dir=eval_out, conf_thr=CONF_THRESHOLD, iou_thr=IOU_THRESHOLD, device=device)\n",
        "save_metrics_row(res, seq_name=\"TRAIN_ALL\", model_tag=\"fasterrcnn_head_finetune\", conf_thr=CONF_THRESHOLD, notes=\"aug+AdamW+warmup_cos\")"
      ],
      "metadata": {
        "id": "N5tLdsUwhDu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237cb466-ea52-4970-af42-86fbaf01f01a"
      },
      "id": "N5tLdsUwhDu9",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Loaded weights: /content/drive/MyDrive/Deep_Learning/Final Project/models_FasterRCNN/bbox_detector.pth\n",
            "[INFO] Eval dataset length: 1500\n",
            "[OK] Writing metrics to: /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/eval_train_conf0.7/metrics.txt\n",
            "[OK] Add metrics to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/metrics_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST VIDEO CREATION\n",
        "# d) Reload weights for inference\n",
        "model = get_detector_model(weights_path=MODEL_PATH)\n",
        "\n",
        "# e) Single sequence or complete sequence (testing = no ground truth)\n",
        "test_seqs = [\"MOT16-01\",\"MOT16-03\",\"MOT16-06\",\"MOT16-07\",\"MOT16-08\",\"MOT16-12\",\"MOT16-14\"]\n",
        "if RUN_ALL_TEST_SEQ:\n",
        "    for seq in test_seqs:\n",
        "        sequence_dir = os.path.join(DATA_DIR, \"test\", seq)\n",
        "        out_dir = os.path.join(RESULT_DIR, f\"test_{seq}\")\n",
        "        text_output = os.path.join(out_dir, f\"test_{seq}.txt\")\n",
        "        fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
        "        video_path = os.path.join(out_dir, f\"test_{seq}.mp4\")\n",
        "        visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
        "else:\n",
        "    seq = TEST_SEQ\n",
        "    sequence_dir = os.path.join(DATA_DIR, \"test\", seq)\n",
        "    out_dir = os.path.join(RESULT_DIR, f\"test_{seq}\")\n",
        "    text_output = os.path.join(out_dir, f\"test_{seq}.txt\")\n",
        "    fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
        "    video_path = os.path.join(out_dir, f\"test_{seq}.mp4\")\n",
        "    visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
        "\n",
        "print(\"\\n[DONE]\")"
      ],
      "metadata": {
        "id": "goAJxqqSV0ZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fccb071-6eb9-4088-b86b-63a935089030"
      },
      "id": "goAJxqqSV0ZX",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Loaded weights: /content/drive/MyDrive/Deep_Learning/Final Project/models_FasterRCNN/bbox_detector.pth\n",
            "[INFO] Running inference on frames：1500 frames: /content/drive/MyDrive/Deep_Learning/Final Project/MOT16/test/MOT16-03\n",
            "[OK] Detections saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/test_MOT16-03\n",
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/test_MOT16-03/test_MOT16-03.mp4\n",
            "\n",
            "[DONE]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING VIDEO CREATION\n",
        "# D) Reload weights for inference\n",
        "model = get_detector_model(weights_path=MODEL_PATH)\n",
        "\n",
        "# Make video\n",
        "seq = \"MOT16-02\" # sequence of frames\n",
        "sequence_dir = os.path.join(DATA_DIR, \"train\", seq)\n",
        "out_dir = os.path.join(RESULT_DIR, f\"train_{seq}\")\n",
        "text_output = os.path.join(out_dir, f\"train_{seq}.txt\")\n",
        "fasterrcnn_detections_to_txt(model, sequence_dir, out_dir, text_output, threshold=CONF_THRESHOLD) # create txt with detections\n",
        "video_path = os.path.join(out_dir, f\"train_{seq}.mp4\")\n",
        "visualize_sequence(sequence_dir, results_file=text_output, output_video_path=video_path) # save video with detections\n",
        "\n",
        "print(\"\\n[DONE]\")"
      ],
      "metadata": {
        "id": "P_H3hkcbnfXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756028e2-10d5-4848-86f1-256fe2d23033"
      },
      "id": "P_H3hkcbnfXd",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Loaded weights: /content/drive/MyDrive/Deep_Learning/Final Project/models_FasterRCNN/bbox_detector.pth\n",
            "[INFO] Running inference on frames：600 frames: /content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train/MOT16-02\n",
            "[OK] Detections saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/train_MOT16-02\n",
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/train_MOT16-02/train_MOT16-02.mp4\n",
            "\n",
            "[DONE]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}