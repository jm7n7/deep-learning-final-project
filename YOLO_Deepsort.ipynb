{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup"
      ],
      "metadata": {
        "id": "xGG-tiDyWAWL"
      },
      "id": "xGG-tiDyWAWL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount users Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "_BWv-saks0Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ee4429-990c-496f-ef52-874d4dece89f"
      },
      "id": "_BWv-saks0Xr",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea44547",
      "metadata": {
        "id": "eea44547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e51d11b-a102-431c-c182-da93796650ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Unzip the MOT16 Dataset if needed\n",
        "!unzip -q \"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16.zip\" -d \"/content/drive/MyDrive/Deep_Learning/Final Project\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "!pip install motmetrics -q\n",
        "!pip install ultralytics -q\n",
        "!pip install deep_sort_realtime -q"
      ],
      "metadata": {
        "id": "hloFXnkgs5AC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a4c922-8b45-42b9-8166-9b732adf648f"
      },
      "id": "hloFXnkgs5AC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0fbc7bdb",
      "metadata": {
        "id": "0fbc7bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf78d944-5c06-4320-a906-3e91cb78b6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# All Imports\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "import yaml\n",
        "import torch\n",
        "import shutil\n",
        "import random\n",
        "import configparser\n",
        "#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import motmetrics as mm\n",
        "import albumentations as A\n",
        "#\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "# Patch np.asfarray if it does not exist (NumPy 2.x) for motmetrics library\n",
        "if not hasattr(np, \"asfarray\"):\n",
        "    np.asfarray = lambda a: np.asarray(a, dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add path and folders\n",
        "sys.path.append(\"/content/drive/MyDrive/Deep_Learning/Final Project/\")\n",
        "save_folder = \"/content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO\"\n",
        "os.makedirs(save_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "ywhvWlmLA9rv"
      },
      "id": "ywhvWlmLA9rv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "# Seed\n",
        "seed=42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "# Data\n",
        "train_val_ratio=0.8\n",
        "# Training Parameters\n",
        "lr=1e-3\n",
        "wd=5e-4\n",
        "EPOCHS=15\n",
        "BATCH_SIZE=2\n",
        "# Validation and metrics\n",
        "CONF_THRESHOLD = 0.7\n",
        "IOU_THRESHOLD = 0.5\n",
        "# Deepsort\n",
        "max_age=10\n",
        "n_init=3\n",
        "max_cosine_distance=0.2\n"
      ],
      "metadata": {
        "id": "v7x1WGJIfqKL"
      },
      "id": "v7x1WGJIfqKL",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO Data Preprocessing"
      ],
      "metadata": {
        "id": "AID-O4MWWFRt"
      },
      "id": "AID-O4MWWFRt"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "418b968e",
      "metadata": {
        "id": "418b968e"
      },
      "outputs": [],
      "source": [
        "# Create Required YOLO Data\n",
        "def create_yolo_labels(source_dir):\n",
        "    for sequence_name in os.listdir(source_dir):\n",
        "        sequence_dir = os.path.join(source_dir, sequence_name)\n",
        "\n",
        "        # Skip if labels already exist\n",
        "        labels_dir = os.path.join(sequence_dir, \"labels\")\n",
        "        if os.path.exists(labels_dir):\n",
        "            print(f\"Skipping {sequence_name} (labels already exist)\")\n",
        "            continue\n",
        "\n",
        "        # Read the sequence information\n",
        "        seqinfo_path = os.path.join(sequence_dir, \"seqinfo.ini\")\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read(seqinfo_path)\n",
        "        imWidth = config.getint(\"Sequence\",\"imWidth\")\n",
        "        imHeight = config.getint(\"Sequence\",\"imHeight\")\n",
        "\n",
        "        # Create new YOLO labels folder\n",
        "        new_labels = os.path.join(sequence_dir,\"labels\")\n",
        "        os.makedirs(new_labels,exist_ok=True)\n",
        "\n",
        "        # Ground Truth location\n",
        "        gt_path = os.path.join(sequence_dir, \"gt\", \"gt.txt\")\n",
        "\n",
        "        # Read ground truth using Pandas\n",
        "        columns = [\"frame\", \"id\", \"bb_left\", \"bb_top\", \"bb_width\", \"bb_height\",\"conf\", \"x\", \"y\", \"z\"]\n",
        "        gt = pd.read_csv(gt_path, header=None, names=columns)\n",
        "\n",
        "        # Filter rows for postive confidence\n",
        "        filter_gt = gt[gt[\"conf\"]>0].copy()\n",
        "\n",
        "        # Calculate required label information for YOLO\n",
        "        filter_gt[\"class_id\"] = 0 # only one class pedestrians\n",
        "        filter_gt[\"x_center\"] = (filter_gt[\"bb_left\"]+filter_gt[\"bb_width\"]/2)/imWidth\n",
        "        filter_gt[\"y_center\"] = (filter_gt[\"bb_top\"]+filter_gt[\"bb_height\"]/2)/imHeight\n",
        "        filter_gt[\"width_norm\"] = filter_gt[\"bb_width\"]/imWidth\n",
        "        filter_gt[\"height_norm\"] = filter_gt[\"bb_height\"]/imHeight\n",
        "\n",
        "        # Clip all values to [0,1] to fix any bounding issues\n",
        "        for col in [\"x_center\", \"y_center\", \"width_norm\", \"height_norm\"]:\n",
        "            filter_gt[col] = filter_gt[col].clip(0.0, 1.0)\n",
        "\n",
        "        # Group ground truth per frame and save 1 file\n",
        "        for frame_ids, group in filter_gt.groupby(\"frame\"):\n",
        "            labels_path=os.path.join(new_labels, f\"{int(frame_ids):06d}.txt\")\n",
        "            group[[\"class_id\", \"x_center\", \"y_center\", \"width_norm\", \"height_norm\"]].to_csv(labels_path, sep= \" \", header=False, index=False, float_format=\"%.4f\")\n",
        "\n",
        "        print(f\"Converted: {sequence_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e03bc2dc",
      "metadata": {
        "id": "e03bc2dc"
      },
      "outputs": [],
      "source": [
        "# Create YOLO File Structure\n",
        "def train_val_split(source_dir, split_ratio, seed):\n",
        "    sequences = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "    sequences.sort()\n",
        "    print(\"All sequences:\", sequences)\n",
        "\n",
        "    random.seed(seed)\n",
        "    random.shuffle(sequences)\n",
        "\n",
        "    split_idx = int(len(sequences) * split_ratio)\n",
        "    train_sequence = sequences[:split_idx]\n",
        "    val_sequence = sequences[split_idx:]\n",
        "\n",
        "    print(\"Training sequences:\", train_sequence)\n",
        "    print(\"Validation sequences:\", val_sequence)\n",
        "\n",
        "    return train_sequence, val_sequence\n",
        "\n",
        "def yolo_folder_structure(target_dir, source_dir, sequence_list, split_type):\n",
        "    # Create empty folders\n",
        "    os.makedirs(os.path.join(target_dir, \"images\", split_type), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, \"labels\", split_type), exist_ok=True)\n",
        "\n",
        "    # Copy images and labels for YOLO folder\n",
        "    for sequence in sequence_list:\n",
        "        img_source = os.path.join(source_dir, sequence, \"img1\")\n",
        "        lbl_source = os.path.join(source_dir, sequence, \"labels\")\n",
        "\n",
        "        img_target = os.path.join(target_dir, \"images\", split_type, sequence)\n",
        "        lbl_target = os.path.join(target_dir, \"labels\", split_type, sequence)\n",
        "\n",
        "        os.makedirs(img_target, exist_ok=True)\n",
        "        os.makedirs(lbl_target, exist_ok=True)\n",
        "\n",
        "        # Copy images\n",
        "        for f in os.listdir(img_source):\n",
        "            shutil.copy(os.path.join(img_source, f), os.path.join(img_target, f))\n",
        "\n",
        "        # Copy labels\n",
        "        for f in os.listdir(lbl_source):\n",
        "            shutil.copy(os.path.join(lbl_source, f), os.path.join(lbl_target, f))\n",
        "\n",
        "    print(f\"{split_type} sequences copied: {sequence_list}\")\n",
        "\n",
        "def create_yolo_data_yaml(data_dir, output_path=\"MOT16.yaml\"):\n",
        "    data_dict = {\n",
        "        \"train\": os.path.join(data_dir, \"images/train\").replace(\"\\\\\",\"/\"),\n",
        "        \"val\": os.path.join(data_dir, \"images/val\").replace(\"\\\\\",\"/\"),\n",
        "        \"nc\": 1,\n",
        "        \"names\": [\"pedestrian\"]\n",
        "    }\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        yaml.dump(data_dict, f)\n",
        "\n",
        "    print(f\"YOLOv8 data.yaml saved as {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3b265d5f",
      "metadata": {
        "id": "3b265d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2633be76-2eb7-4286-ebca-a1ddab0df017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping MOT16-13 (labels already exist)\n",
            "Skipping MOT16-09 (labels already exist)\n",
            "Skipping MOT16-10 (labels already exist)\n",
            "Skipping MOT16-11 (labels already exist)\n",
            "Skipping MOT16-05 (labels already exist)\n",
            "Skipping MOT16-02 (labels already exist)\n",
            "Skipping MOT16-04 (labels already exist)\n",
            "All sequences: ['MOT16-02', 'MOT16-04', 'MOT16-05', 'MOT16-09', 'MOT16-10', 'MOT16-11', 'MOT16-13']\n",
            "Training sequences: ['MOT16-04', 'MOT16-09', 'MOT16-10', 'MOT16-05', 'MOT16-13']\n",
            "Validation sequences: ['MOT16-02', 'MOT16-11']\n",
            "train sequences copied: ['MOT16-04', 'MOT16-09', 'MOT16-10', 'MOT16-05', 'MOT16-13']\n",
            "val sequences copied: ['MOT16-02', 'MOT16-11']\n",
            "YOLOv8 data.yaml saved as MOT16.yaml\n"
          ]
        }
      ],
      "source": [
        "# Prepare the data\n",
        "create_yolo_labels(source_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train\") # Create Labels\n",
        "train_sequence, val_sequence = train_val_split(source_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train\", split_ratio=train_val_ratio, seed=seed) # Split Data\n",
        "yolo_folder_structure(target_dir=\"MOT16_YOLO\", source_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train\", sequence_list=train_sequence, split_type=\"train\") # Store training data\n",
        "yolo_folder_structure(target_dir=\"MOT16_YOLO\", source_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train\", sequence_list=val_sequence, split_type=\"val\") # Store validation data\n",
        "create_yolo_data_yaml(data_dir=\"MOT16_YOLO\") # Create YAML file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detector Training and Validation"
      ],
      "metadata": {
        "id": "s8XTlpEKWfl_"
      },
      "id": "s8XTlpEKWfl_"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9d8dfdab",
      "metadata": {
        "id": "9d8dfdab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79230c1-cc1c-461c-e821-5ee7abe5c31b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 21.5MB 518.1MB/s 0.0s\n",
            "Ultralytics 8.3.235 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, augmentations=[ColorJitter(p=0.5, brightness=(0.8, 1.2), contrast=(0.8, 1.2), hue=(-0.2, 0.2), saturation=(0.8, 1.2)), GaussianBlur(p=0.25, blur_limit=(3, 5), sigma_limit=(0.1, 1.0))], auto_augment=None, batch=2, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=MOT16.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.25, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.15, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 178.9MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 349.3MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2830.2Â±1291.0 MB/s, size: 174.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/MOT16_YOLO/labels/train/MOT16-04... 3816 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3816/3816 1.5Kit/s 2.6s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/MOT16_YOLO/labels/train/MOT16-04.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mColorJitter(p=0.5, brightness=(0.8, 1.2), contrast=(0.8, 1.2), hue=(-0.2, 0.2), saturation=(0.8, 1.2)), GaussianBlur(p=0.25, blur_limit=(3, 5), sigma_limit=(0.1, 1.0))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1609.2Â±1160.3 MB/s, size: 161.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/MOT16_YOLO/labels/val/MOT16-02... 1500 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1500/1500 1.2Kit/s 1.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/MOT16_YOLO/labels/val/MOT16-02.cache\n",
            "Plotting labels to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/15     0.703G      1.408     0.9009      1.126         54        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 15.4it/s 2:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 25.5it/s 14.7s\n",
            "                   all       1500      27007      0.782      0.433      0.529      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/15     0.777G      1.233      0.731       1.05         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.6it/s 1:55\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 26.3it/s 14.3s\n",
            "                   all       1500      27007      0.773      0.418      0.522      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/15     0.838G      1.165     0.6729       1.02         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.6it/s 1:55\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.3it/s 13.8s\n",
            "                   all       1500      27007      0.803      0.402      0.501      0.306\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/15     0.895G      1.112     0.6347     0.9997         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 26.7it/s 14.1s\n",
            "                   all       1500      27007      0.743      0.417      0.503      0.296\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/15     0.977G      1.072     0.6087     0.9881         83        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.3it/s 13.7s\n",
            "                   all       1500      27007      0.809      0.404      0.498      0.308\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mColorJitter(p=0.5, brightness=(0.8, 1.2), contrast=(0.8, 1.2), hue=(-0.2, 0.2), saturation=(0.8, 1.2)), GaussianBlur(p=0.25, blur_limit=(3, 5), sigma_limit=(0.1, 1.0))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/15      1.04G      1.035     0.5805     0.9702         52        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 26.8it/s 14.0s\n",
            "                   all       1500      27007      0.814      0.404      0.523      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/15       1.1G     0.9986     0.5597     0.9573         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.6it/s 1:55\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.4it/s 13.7s\n",
            "                   all       1500      27007      0.813      0.391      0.507       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/15      1.17G     0.9635     0.5394     0.9476         89        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.3it/s 13.8s\n",
            "                   all       1500      27007      0.787        0.4      0.523      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/15      1.23G     0.9418     0.5243     0.9414         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.6it/s 1:55\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.4it/s 13.7s\n",
            "                   all       1500      27007      0.821      0.379      0.484      0.304\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/15       1.3G     0.9117     0.5088     0.9297         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.5it/s 13.6s\n",
            "                   all       1500      27007      0.808      0.401      0.512      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/15      1.37G     0.8854     0.4906     0.9207         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 28.0it/s 13.4s\n",
            "                   all       1500      27007      0.848      0.376        0.5      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/15      1.44G     0.8589     0.4784     0.9152         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.7it/s 1:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.8it/s 13.5s\n",
            "                   all       1500      27007      0.815      0.373      0.487      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/15       1.5G     0.8379     0.4639     0.9058         51        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.9it/s 1:53\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.7it/s 13.5s\n",
            "                   all       1500      27007      0.837      0.393      0.507      0.313\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/15      1.57G     0.8176     0.4532     0.8997         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.9it/s 1:53\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.7it/s 13.5s\n",
            "                   all       1500      27007      0.809      0.391       0.51      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/15      1.63G     0.7928     0.4398     0.8931         67        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1908/1908 16.9it/s 1:53\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 27.9it/s 13.5s\n",
            "                   all       1500      27007      0.845      0.378      0.512      0.317\n",
            "\n",
            "15 epochs completed in 0.538 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train/weights/best.pt...\n",
            "Ultralytics 8.3.235 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 375/375 39.0it/s 9.6s\n",
            "                   all       1500      27007      0.787        0.4      0.523      0.327\n",
            "Speed: 0.1ms preprocess, 1.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "model = YOLO(\"yolov8s.pt\") # load model\n",
        "results = model.train(data=\"MOT16.yaml\", optimizer=\"AdamW\", lr0=lr, weight_decay=wd, epochs = EPOCHS, batch=BATCH_SIZE, seed=seed, project = save_folder, # data augmentation and train the model\n",
        "                      auto_augment=None, translate=.15, scale=0.25, fliplr=0.5, mosaic=0,\n",
        "                      augmentations=[A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5), A.GaussianBlur(blur_limit=(3,5), sigma_limit=(0.1, 1.0), p=0.25)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "79fbd739",
      "metadata": {
        "id": "79fbd739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b57cde-97f9-4b09-8368-13ecf617fb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.235 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2775.3Â±1232.8 MB/s, size: 160.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/MOT16_YOLO/labels/val/MOT16-02.cache... 1500 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1500/1500 3.4Mit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 94/94 10.9it/s 8.6s\n",
            "                   all       1500      27007      0.994      0.224      0.609      0.471\n",
            "Speed: 0.4ms preprocess, 1.1ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/val\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "metrics = model.val(data=\"MOT16.yaml\", conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, project = save_folder)\n",
        "csv_string=metrics.to_csv()\n",
        "csv_path = os.path.join(save_folder, \"Evaluation_metrics.csv\")\n",
        "with open(csv_path, \"w\") as f:\n",
        "    f.write(csv_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detector Visualization"
      ],
      "metadata": {
        "id": "TDCKVRcBY2Ln"
      },
      "id": "TDCKVRcBY2Ln"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bf35ba6f",
      "metadata": {
        "id": "bf35ba6f"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "def yolo_detections_to_txt(frames_folder, model_path, output_txt_path, conf, iou):\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    frame_files = sorted([f for f in os.listdir(frames_folder) if f.endswith(\".jpg\")])\n",
        "\n",
        "    # Open Detector txt file\n",
        "    with open(output_txt_path, \"w\") as f_out:\n",
        "        for frame_idx, frame_file in enumerate(frame_files, start=1):\n",
        "            frame_path = os.path.join(frames_folder, frame_file)\n",
        "            frame = cv2.imread(frame_path)\n",
        "\n",
        "            # YOLO detections\n",
        "            results = model.predict(frame, conf=conf, iou=iou, verbose=False)[0]\n",
        "            for box, conf_tensor in zip(results.boxes.xyxy, results.boxes.conf): # xyxy = Boxes in pixel format [x1, y1, x2, y2]\n",
        "                x1, y1, x2, y2 = box.tolist() # tensor to list and x1,y1 = top left corner and x2,y2 = bottom right corner\n",
        "                w, h = x2-x1, y2-y1\n",
        "                conf_val= float(conf_tensor)\n",
        "                track_id=0 # placeholder ids because detection only\n",
        "\n",
        "            # Write detections to Detector txt file\n",
        "                f_out.write(f\"{frame_idx},{track_id},{int(x1)},{int(y1)},{int(w)},{int(h)},{conf_val},-1,-1,-1\\n\")\n",
        "    print(f\"Detected results saved to {output_txt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "def get_sequence_info(sequence_dir):\n",
        "    seqinfo_path = os.path.join(sequence_dir, \"seqinfo.ini\")\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(seqinfo_path)\n",
        "    fps = None\n",
        "    width = None\n",
        "    height = None\n",
        "    fps = config.getint(\"Sequence\", \"frameRate\", fallback=fps)\n",
        "    width = config.getint(\"Sequence\", \"imWidth\", fallback=width)\n",
        "    height = config.getint(\"Sequence\", \"imHeight\", fallback=height)\n",
        "    return fps, (width, height)\n",
        "\n",
        "def visualize_sequence(sequence_dir, results_file, output_video_path):\n",
        "    # Load results\n",
        "    results_df = pd.read_csv(results_file, header=None)\n",
        "    results_df.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"x3\",\"y3\",\"z3\"]\n",
        "\n",
        "    # Load frames\n",
        "    image_paths = sorted(glob.glob(os.path.join(sequence_dir, \"img1\", \"*.jpg\")))\n",
        "\n",
        "    # Video writer\n",
        "    fps, frame_size = get_sequence_info(sequence_dir)\n",
        "    first_img = cv2.imread(image_paths[0])\n",
        "    frame_size = (first_img.shape[1], first_img.shape[0])\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
        "\n",
        "    # Assign random colors for IDs\n",
        "    max_id = int(results_df[\"id\"].max()) + 1\n",
        "    np.random.seed(42)  # reproducible colors\n",
        "    colors = np.random.randint(0, 255, size=(max_id, 3), dtype=np.uint8)\n",
        "\n",
        "    # Process frames\n",
        "    for img_path in image_paths:\n",
        "        frame_num = int(os.path.splitext(os.path.basename(img_path))[0])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        frame_data = results_df[results_df[\"frame\"] == frame_num]\n",
        "        for _, row in frame_data.iterrows():\n",
        "            track_id = int(row[\"id\"])\n",
        "            color = tuple(map(int, colors[track_id]))\n",
        "            x, y, w_box, h_box = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
        "            cv2.rectangle(img, (x, y), (x + w_box, y + h_box), color, 2)\n",
        "            cv2.putText(img, str(track_id), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        video_writer.write(img)\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {output_video_path}\")"
      ],
      "metadata": {
        "id": "OfNy-Ar7P7n6"
      },
      "id": "OfNy-Ar7P7n6",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect pedestrians\n",
        "best_model = os.path.join(save_folder, \"train/weights/best.pt\")\n",
        "det_output_path = os.path.join(save_folder, \"Detect.txt\")\n",
        "yolo_detections_to_txt(frames_folder=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train/MOT16-02/img1/\", model_path=best_model, output_txt_path=det_output_path, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLyWsd8vYzCX",
        "outputId": "0e8256ef-b705-4190-dd23-4026c9420e43"
      },
      "id": "MLyWsd8vYzCX",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected results saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/Detect.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "det_video_path = os.path.join(save_folder, \"Detect.mp4\")\n",
        "visualize_sequence(sequence_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train/MOT16-02/\", results_file=det_output_path, output_video_path=det_video_path)"
      ],
      "metadata": {
        "id": "_YFvVhqO6mFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9d5dea-0a1f-44ff-ed51-d56097d99782"
      },
      "id": "_YFvVhqO6mFD",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/Detect.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ID Tracking"
      ],
      "metadata": {
        "id": "VcpICWL5Wup9"
      },
      "id": "VcpICWL5Wup9"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d4665db3",
      "metadata": {
        "id": "d4665db3"
      },
      "outputs": [],
      "source": [
        "# Deepsort\n",
        "def track_frames_to_mot_txt(frames_folder, model_path, output_txt_path, conf, iou, max_age, n_init, max_cosine_distance):\n",
        "    # Load YOLOv8\n",
        "    model = YOLO(model_path)\n",
        "    tracker = DeepSort(\n",
        "        max_age= max_age,                       # how long to keep a lost track\n",
        "        n_init=n_init,                          # frames to confirm a track\n",
        "        max_cosine_distance=max_cosine_distance # threshold for re-ID feature matching\n",
        "    )\n",
        "\n",
        "    # Sort frame files\n",
        "    frame_files = sorted([f for f in os.listdir(frames_folder) if f.endswith(\".jpg\")])\n",
        "\n",
        "    # Open MOT txt file\n",
        "    with open(output_txt_path, \"w\") as f_out:\n",
        "        for frame_idx, frame_file in enumerate(frame_files, start=1):\n",
        "            frame_path = os.path.join(frames_folder, frame_file)\n",
        "            frame = cv2.imread(frame_path)\n",
        "\n",
        "            # YOLO detections\n",
        "            results = model.predict(frame, conf=conf, iou=iou, verbose=False, seed=42)[0]\n",
        "            detections = []\n",
        "            for box, conf_tensor in zip(results.boxes.xyxy, results.boxes.conf): # xyxy = Boxes in pixel format [x1, y1, x2, y2]\n",
        "                x1, y1, x2, y2 = box.tolist() # tensor to list and x1,y1 = top left corner and x2,y2 = bottom right corner\n",
        "                w, h = x2-x1, y2-y1\n",
        "                detections.append(([x1, y1, w, h], float(conf_tensor))) # DeepSORT expects ([x, y, w, h], conf)\n",
        "\n",
        "            # Update DeepSORT\n",
        "            tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "            # Write tracks to MOT txt\n",
        "            for track in tracks:\n",
        "                x, y, w, h = track.to_tlwh()  # top-left, width-height\n",
        "                track_id = track.track_id\n",
        "                conf_val = 1.0  # placeholder, tracker does not store confidence\n",
        "                f_out.write(f\"{frame_idx},{track_id},{int(x)},{int(y)},{int(w)},{int(h)},{conf_val},-1,-1,-1\\n\")\n",
        "\n",
        "    print(f\"Tracking results saved to {output_txt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "12fdd722",
      "metadata": {
        "id": "12fdd722"
      },
      "outputs": [],
      "source": [
        "# Tracking Metrics\n",
        "def evaluate_mot_sequence(gt_txt_path, tracker_txt_path, csv_path, max_iou=0.5):\n",
        "    # Load GT and tracker files\n",
        "    gt = pd.read_csv(gt_txt_path, header=None)\n",
        "    tr = pd.read_csv(tracker_txt_path, header=None)\n",
        "\n",
        "    # Keep only first 7 columns: frame, id, x, y, w, h, conf\n",
        "    gt = gt.iloc[:, :7]\n",
        "    tr = tr.iloc[:, :7]\n",
        "\n",
        "    gt.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
        "    tr.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
        "\n",
        "    gt = gt[gt[\"conf\"] > 0].copy() # keep only evaluation boxes (conf > 0)\n",
        "    tr = tr[tr[\"conf\"] > 0].copy() # keep only evaluation boxes (conf > 0)\n",
        "\n",
        "    # Initialize MOTAccumulator\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "\n",
        "    # Go frame by frame\n",
        "    frames = sorted(gt[\"frame\"].unique())\n",
        "    for f in frames:\n",
        "        gt_frame = gt[gt[\"frame\"]==f]\n",
        "        tr_frame = tr[tr[\"frame\"]==f]\n",
        "\n",
        "        gt_ids = gt_frame[\"id\"].tolist()\n",
        "        tr_ids = tr_frame[\"id\"].tolist()\n",
        "\n",
        "        gt_boxes = gt_frame[[\"x\",\"y\",\"w\",\"h\"]].values\n",
        "        tr_boxes = tr_frame[[\"x\",\"y\",\"w\",\"h\"]].values\n",
        "\n",
        "        # Compute IoU distance matrix\n",
        "        distances = 1 - mm.distances.iou_matrix(gt_boxes, tr_boxes, max_iou=max_iou)\n",
        "        acc.update(gt_ids, tr_ids, distances)\n",
        "\n",
        "    # Create MetricsHost\n",
        "    mh = mm.metrics.create()\n",
        "    # Compute MOTChallenge metrics\n",
        "    summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics)\n",
        "    summary.to_csv(csv_path)\n",
        "    print(f\"[OK] Metrics saved to {csv_path}\")\n",
        "    print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "55a83aa1",
      "metadata": {
        "id": "55a83aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a472b1-e2f8-4101-c572-7f798329df34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking results saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/track.txt\n"
          ]
        }
      ],
      "source": [
        "# Track id accross frames using DeepSort and save detections in MOT format\n",
        "id_output_path = os.path.join(save_folder, \"track.txt\")\n",
        "track_frames_to_mot_txt(frames_folder=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train/MOT16-02/img1/\",model_path=best_model,output_txt_path=id_output_path, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, max_age=max_age, n_init=n_init, max_cosine_distance=max_cosine_distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7adb7353",
      "metadata": {
        "id": "7adb7353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dc2bd9-4e91-49b1-e951-d4a71c62c3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Metrics saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/MOT.csv\n",
            "       idf1       idp       idr    recall  precision  num_unique_objects  \\\n",
            "0  0.148486  0.556041  0.085684  0.150788    0.97853                  54   \n",
            "\n",
            "   mostly_tracked  partially_tracked  mostly_lost  num_false_positives  \\\n",
            "0               4                 11           39                   59   \n",
            "\n",
            "   num_misses  num_switches  num_fragmentations      mota      motp  \\\n",
            "0       15144            76                  75  0.143218  0.786879   \n",
            "\n",
            "   num_transfer  num_ascend  num_migrate  \n",
            "0            15          65            4  \n"
          ]
        }
      ],
      "source": [
        "# Compute MOT metrics on DeepSort Result\n",
        "metrics_output_path = os.path.join(save_folder, \"MOT.csv\")\n",
        "evaluate_mot_sequence(gt_txt_path=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train/MOT16-02/gt/gt.txt\", tracker_txt_path=id_output_path, csv_path=metrics_output_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking video\n",
        "video_output_path = os.path.join(save_folder, \"tracking_video.mp4\")\n",
        "visualize_sequence(sequence_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/train/MOT16-02/\", results_file=id_output_path, output_video_path=video_output_path)"
      ],
      "metadata": {
        "id": "jW4jbbuZ031H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7715be2-1d39-4c4e-a05d-76a055501e3a"
      },
      "id": "jW4jbbuZ031H",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/tracking_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing video tracking"
      ],
      "metadata": {
        "id": "Rs-9LPEP1tNv"
      },
      "id": "Rs-9LPEP1tNv"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8990f164",
      "metadata": {
        "id": "8990f164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a73ea3c-e888-469b-879e-489f3f5733e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking results saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/track_TEST3.txt\n"
          ]
        }
      ],
      "source": [
        "# Run Deepsort on testing video\n",
        "testID_output_path = os.path.join(save_folder, \"track_TEST3.txt\")\n",
        "track_frames_to_mot_txt(frames_folder=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/test/MOT16-03/img1/\",model_path=best_model,output_txt_path=testID_output_path, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, max_age=max_age, n_init=n_init, max_cosine_distance=max_cosine_distance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Video with tracking\n",
        "test_video_output_path = os.path.join(save_folder, \"test_tracking_video_03.mp4\")\n",
        "visualize_sequence(sequence_dir=\"/content/drive/MyDrive/Deep_Learning/Final Project/MOT16/test/MOT16-03/\", results_file=testID_output_path, output_video_path=test_video_output_path)"
      ],
      "metadata": {
        "id": "MSdag-sSSRGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8112e257-45e4-4e1a-f1a6-56ee561cd925"
      },
      "id": "MSdag-sSSRGv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_YOLO/test_tracking_video_03.mp4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}