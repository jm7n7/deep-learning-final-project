{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U0Rxv4MSzw4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bc965f-2e25-4205-84ff-6c479989ce30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Deep_Learning/Final Project')"
      ],
      "metadata": {
        "id": "BqOUSvxUjZKZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the root folder withing your Google Drive for this project (!User must change based on their own setup!)\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Deep_Learning/Final Project\""
      ],
      "metadata": {
        "id": "dkZFKe53hF8Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install motmetrics -q\n",
        "!pip install ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9CkzYpjjfTH",
        "outputId": "f551e24f-9eea-45b5-bff6-823b07e4825e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipynb in /usr/local/lib/python3.12/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_Learning/Final Project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bgM02EVlpHt",
        "outputId": "2622a08a-6301-47e8-c437-7b8f7d3ec7ed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_Learning/Final Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QkN7AnlOyJ84"
      },
      "outputs": [],
      "source": [
        "# Import All Packages\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import configparser\n",
        "#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import motmetrics as mm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "#\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from random import randint\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image, write_jpeg, write_video\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "#\n",
        "# Patch np.asfarray if it does not exist (NumPy 2.x) for motmetrics library\n",
        "if not hasattr(np, \"asfarray\"):\n",
        "    np.asfarray = lambda a: np.asarray(a, dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Siamese Network Class\n",
        "class Siamese_Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Siamese_Network, self).__init__()\n",
        "        # Input size: 3x128x64\n",
        "\n",
        "        # Update the input channels of conv1 from 1 to 3\n",
        "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3)\n",
        "        self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=3)\n",
        "        self.conv3 = torch.nn.Conv2d(128, 128, kernel_size=3)\n",
        "\n",
        "        # 2 fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(128*14*6, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "        x = torch.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        return self.forward_one(x1), self.forward_one(x2)"
      ],
      "metadata": {
        "id": "1I3siQUe13Bs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gallery Class\n",
        "class Gallery(torch.utils.data.Dataset):\n",
        "    errorCount = 0\n",
        "    def __init__(self, path, transform=None, max_pairs_per_id=50, max_neg_pairs_per_id=50):\n",
        "        self.path = path\n",
        "        self.imgs = sorted([x for x in os.listdir(path) if x.endswith('.jpg')])\n",
        "        self.transform = transform\n",
        "        self.max_pairs_per_id = max_pairs_per_id  # Limit positive pairs\n",
        "        self.max_neg_pairs_per_id = max_neg_pairs_per_id  # Limit negative pairs\n",
        "        self.pairs = []\n",
        "        self.labels = []\n",
        "        self._create_pairs()\n",
        "\n",
        "    def _create_pairs(self):\n",
        "        # Organize images by person ID\n",
        "        person_images = {}\n",
        "        for img_name in self.imgs:\n",
        "            person_id = img_name.split('_')[0]\n",
        "            if person_id not in person_images:\n",
        "                person_images[person_id] = []\n",
        "            person_images[person_id].append(img_name)\n",
        "\n",
        "        # Create pairs\n",
        "        for person_id, images in person_images.items():\n",
        "            # Positive pairs: randomly choose up to `max_pairs_per_id` pairs\n",
        "            positive_pairs = [(images[i], images[j]) for i in range(len(images)) for j in range(i + 1, len(images))]\n",
        "            positive_pairs = random.sample(positive_pairs, min(len(positive_pairs), self.max_pairs_per_id))\n",
        "            self.pairs.extend(positive_pairs)\n",
        "            self.labels.extend([1] * len(positive_pairs))\n",
        "\n",
        "            # Negative pairs: choose random images from other person IDs\n",
        "            other_person_ids = list(person_images.keys())\n",
        "            other_person_ids.remove(person_id)\n",
        "            negative_pairs = []\n",
        "            for other_id in random.sample(other_person_ids, min(len(other_person_ids), self.max_neg_pairs_per_id)):\n",
        "                negative_pairs.append((images[0], person_images[other_id][0]))\n",
        "            self.pairs.extend(negative_pairs)\n",
        "            self.labels.extend([0] * len(negative_pairs))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_name, img2_name = self.pairs[idx]\n",
        "        img1_path = os.path.join(self.path, img1_name)\n",
        "        img2_path = os.path.join(self.path, img2_name)\n",
        "\n",
        "        try:\n",
        "            img1 = read_image(img1_path).float() / 255.0\n",
        "            img2 = read_image(img2_path).float() / 255.0\n",
        "        except (RuntimeError, OSError) as e:\n",
        "            Gallery.errorCount += 1\n",
        "            print(f\"Error#: {Gallery.errorCount}. Error loading {img1_path} or {img2_path}: {e}\")\n",
        "            # You can skip, replace, or take another action here.\n",
        "            img1 = torch.zeros(3, 128, 64)  # Placeholder image\n",
        "            img2 = torch.zeros(3, 128, 64)\n",
        "\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "        return img1, img2, label"
      ],
      "metadata": {
        "id": "MoArviAo17Z5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOT16 Train Class\n",
        "class MOT16TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.root = root\n",
        "        # Get image directory paths\n",
        "        self.imgs = []\n",
        "        self.targets = []\n",
        "        for subdir in os.listdir(root):\n",
        "\n",
        "            next_dir = list(sorted(os.listdir(os.path.join(root, subdir, \"img1\"))))\n",
        "            next_dir = [os.path.join(subdir, \"img1\", filename) for filename in next_dir]\n",
        "            self.imgs += next_dir\n",
        "\n",
        "            # Import ground truth data\n",
        "            gt = np.genfromtxt(os.path.join(root, subdir, \"gt\", \"gt.txt\"), delimiter=',', dtype=int)\n",
        "\n",
        "            # Only get the boxes corresponding to people\n",
        "            person_mask = (gt[:, PERSON_COL] == 1)\n",
        "            gt = gt[person_mask, :]\n",
        "\n",
        "            # Get bounding boxes in correct format for pretrained model\n",
        "            # NOTE the \"top\" column in the text file is actually the bottom of the box (y axis is inverted)\n",
        "            bots = gt[:, TOP_COL]\n",
        "            lefts = gt[:, LEFT_COL]\n",
        "            tops = bots + gt[:, HEIGHT_COL]\n",
        "            rights = lefts + gt[:, WIDTH_COL]\n",
        "            boxes = np.column_stack((lefts, bots, rights, tops))\n",
        "            person_ids = gt[:, ID_COL]\n",
        "\n",
        "            # Create target dictionaries for fine tuning\n",
        "            for i in range(len(next_dir)):\n",
        "                d = {}\n",
        "                mask = (gt[:, FRAME_COL] == i+1)\n",
        "                req_boxes = boxes[mask, :]\n",
        "                req_person_ids = person_ids[mask]\n",
        "                d['boxes'] = torch.tensor(req_boxes, dtype=torch.float)\n",
        "                d['labels'] = torch.ones(mask.shape, dtype=torch.int64)\n",
        "                d['person_ids'] = torch.tensor(req_person_ids, dtype=torch.int64)\n",
        "                self.targets.append(d)\n",
        "\n",
        "        print('Length of dataset: ', len(self.imgs))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if(idx > len(self.imgs)):\n",
        "            return None, None\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.root, self.imgs[idx])\n",
        "\n",
        "        # Scale image values between 0 and 1\n",
        "        img = torch.div(read_image(img_path).float(), 255.0)\n",
        "\n",
        "        return img, self.targets[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "QrKjOsTt2GC4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOT16 Test Class\n",
        "class MOT16TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.root = root\n",
        "        # Get image directory path\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"img1\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if(idx > len(self.imgs)):\n",
        "            return None, None\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.root, \"img1\", self.imgs[idx])\n",
        "\n",
        "        # Scale image values between 0 and 1\n",
        "        img = torch.div(read_image(img_path).float(), 255.0)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "IHtC1h0u2Mb_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Detector Model Function\n",
        "def get_detector_model(load_weights=True):\n",
        "    # Configure model with new box predictor head to train\n",
        "    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
        "    if load_weights:\n",
        "        if os.path.exists(model_param_path):\n",
        "            model.load_state_dict(torch.load(model_param_path, weights_only=True))\n",
        "    return model"
      ],
      "metadata": {
        "id": "wiztbc5S2QVe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX41hlx3zvGH"
      },
      "outputs": [],
      "source": [
        "# Unzip MOT16 data if needed\n",
        "!unzip -q f\"{PROJECT_PATH}/MOT16.zip\" -d /content/MOT16_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7V9VPDOtyuOr"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Define constants and paths\n",
        "similarity_model_path = f'{PROJECT_PATH}/siamese_network.pth'\n",
        "sequence_dir = f'{PROJECT_PATH}/MOT16/test/MOT16-03'\n",
        "video_path = f'{PROJECT_PATH}/results_FasterRCNN/tracked_test_video.mp4'\n",
        "model_param_path = f'{PROJECT_PATH}/models_FasterRCNN/bbox_detector.pth'\n",
        "output_txt_path = f'{PROJECT_PATH}/results_FasterRCNN/FasterRCNN_tracker_results.txt'\n",
        "gt_txt_path=os.path.join(sequence_dir,\"gt/gt.txt\")\n",
        "csv_path= f'{PROJECT_PATH}/results_FasterRCNN/FasterRCNN_tracker_metrics.csv'\n",
        "BBOX_SCORE_THRESH = 0.7\n",
        "NUM_CLASSES = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SKUpW-fJyu8-"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Visualization\n",
        "def get_sequence_info(sequence_dir):\n",
        "    seqinfo_path = os.path.join(sequence_dir, \"seqinfo.ini\")\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(seqinfo_path)\n",
        "    fps = None\n",
        "    width = None\n",
        "    height = None\n",
        "    fps = config.getint(\"Sequence\", \"frameRate\", fallback=fps)\n",
        "    width = config.getint(\"Sequence\", \"imWidth\", fallback=width)\n",
        "    height = config.getint(\"Sequence\", \"imHeight\", fallback=height)\n",
        "    return fps, (width, height)\n",
        "\n",
        "def visualize_sequence(sequence_dir, results_file, output_video_path):\n",
        "    # Load results\n",
        "    results_df = pd.read_csv(results_file, header=None)\n",
        "    results_df.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"x3\",\"y3\",\"z3\"]\n",
        "\n",
        "    # Load frames\n",
        "    image_paths = sorted(glob.glob(os.path.join(sequence_dir, \"img1\", \"*.jpg\")))\n",
        "\n",
        "    # Video writer\n",
        "    fps, frame_size = get_sequence_info(sequence_dir)\n",
        "    first_img = cv2.imread(image_paths[0])\n",
        "    frame_size = (first_img.shape[1], first_img.shape[0])\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
        "\n",
        "    # Assign random colors for IDs\n",
        "    max_id = int(results_df[\"id\"].max()) + 1\n",
        "    np.random.seed(42)  # reproducible colors\n",
        "    colors = np.random.randint(0, 255, size=(max_id, 3), dtype=np.uint8)\n",
        "\n",
        "    # Process frames\n",
        "    for img_path in image_paths:\n",
        "        frame_num = int(os.path.splitext(os.path.basename(img_path))[0])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        frame_data = results_df[results_df[\"frame\"] == frame_num]\n",
        "        for _, row in frame_data.iterrows():\n",
        "            track_id = int(row[\"id\"])\n",
        "            color = tuple(map(int, colors[track_id]))\n",
        "            x, y, w_box, h_box = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
        "            cv2.rectangle(img, (x, y), (x + w_box, y + h_box), color, 2)\n",
        "            cv2.putText(img, str(track_id), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        video_writer.write(img)\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {output_video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Evaluation metrics\n",
        "def evaluate_mot_sequence(gt_txt_path, output_txt_path, csv_path, max_iou=0.5):\n",
        "    # Load GT and tracker files\n",
        "    gt = pd.read_csv(gt_txt_path, header=None)\n",
        "    tr = pd.read_csv(output_txt_path, header=None)\n",
        "\n",
        "    # Keep only first 7 columns: frame, id, x, y, w, h, conf\n",
        "    gt = gt.iloc[:, :7]\n",
        "    tr = tr.iloc[:, :7]\n",
        "\n",
        "    gt.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
        "    tr.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
        "\n",
        "    gt = gt[gt[\"conf\"] > 0].copy() # keep only evaluation boxes (conf > 0)\n",
        "    tr = tr[tr[\"conf\"] > 0].copy() # keep only evaluation boxes (conf > 0)\n",
        "\n",
        "    # Initialize MOTAccumulator\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "\n",
        "    # Go frame by frame\n",
        "    frames = sorted(gt[\"frame\"].unique())\n",
        "    for f in frames:\n",
        "        gt_frame = gt[gt[\"frame\"]==f]\n",
        "        tr_frame = tr[tr[\"frame\"]==f]\n",
        "\n",
        "        gt_ids = gt_frame[\"id\"].tolist()\n",
        "        tr_ids = tr_frame[\"id\"].tolist()\n",
        "\n",
        "        gt_boxes = gt_frame[[\"x\",\"y\",\"w\",\"h\"]].values\n",
        "        tr_boxes = tr_frame[[\"x\",\"y\",\"w\",\"h\"]].values\n",
        "\n",
        "        # Compute IoU distance matrix\n",
        "        distances = 1 - mm.distances.iou_matrix(gt_boxes, tr_boxes, max_iou=max_iou)\n",
        "        acc.update(gt_ids, tr_ids, distances)\n",
        "\n",
        "    # Create MetricsHost\n",
        "    mh = mm.metrics.create()\n",
        "    # Compute MOTChallenge metrics\n",
        "    summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics)\n",
        "    summary.to_csv(csv_path)\n",
        "    print(f\"[OK] Metrics saved to {csv_path}\")\n",
        "    print(summary)"
      ],
      "metadata": {
        "id": "3q7y8pGFUOZq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dE211kuyyyf2"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Function to resize image using adaptive pooling\n",
        "def resize_image(image):\n",
        "    # Resize the image to 3x128x64\n",
        "    return F.adaptive_avg_pool2d(image, (128, 64))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    bbox_detector = get_detector_model(load_weights=False)\n",
        "    bbox_detector.load_state_dict(torch.load(model_param_path))\n",
        "    bbox_detector.to(device)\n",
        "    bbox_detector.eval()\n",
        "\n",
        "    similarity_model = Siamese_Network().to(device)\n",
        "    similarity_model.load_state_dict(torch.load(similarity_model_path, weights_only=True))\n",
        "    similarity_model.eval()\n",
        "    similarity_model = Siamese_Network_Extended(similarity_model)\n",
        "\n",
        "    img_set = MOT16TestDataset(sequence_dir)\n",
        "\n",
        "    # Tracking state management\n",
        "    next_id = 0\n",
        "    tracked_persons = {}  # {id: {\"embeddings\": [last N frame embeddings], \"color\": (R,G,B), \"last_seen\": frame_idx}}\n",
        "    MAX_EMBEDDING_HISTORY = 5  # Keep embeddings from last N frames\n",
        "    MAX_FRAMES_MISSING = 30    # Max frames allowed missing\n",
        "    SIMILARITY_THRESHOLD = 0.6 # Threshold below which it is considered a new target\n",
        "\n",
        "\n",
        "    output_file = open(output_txt_path, \"w\")\n",
        "\n",
        "    # tqdm progress display\n",
        "    for i, img in tqdm(enumerate(img_set), total=len(img_set), desc=\"Processing frames\", unit=\"frame\"):\n",
        "        img_device = img.to(device)\n",
        "\n",
        "        # Object detection\n",
        "        bbox_pred = bbox_detector([img_device])[0]\n",
        "        scores = bbox_pred[\"scores\"]\n",
        "        boxes = bbox_pred[\"boxes\"]\n",
        "        # 只保留高于阈值的框及对应的 score\n",
        "        mask = (scores[:] >= BBOX_SCORE_THRESH)\n",
        "        boxes = boxes[mask].type(torch.int32)\n",
        "        scores = scores[mask]\n",
        "\n",
        "        # 当前帧的检测结果（box + embedding + score）\n",
        "        current_detections = []\n",
        "        for box, score in zip(boxes, scores):\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            if x2 - x1 > 0 and y2 - y1 > 0:  # 保证 crop 合法\n",
        "                cropped = resize_image(img_device[:, y1:y2, x1:x2]).to(device)\n",
        "                embedding = similarity_model.get_embedding(cropped)\n",
        "                current_detections.append(\n",
        "                    {\n",
        "                        \"box\": box,\n",
        "                        \"embedding\": embedding,\n",
        "                        \"score\": float(score.item()),  # 检测置信度\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # Match existing tracked targets\n",
        "        unmatched_detections = list(range(len(current_detections)))\n",
        "        matched_tracks = {}\n",
        "\n",
        "        # Try to find best match for each tracked person\n",
        "        for person_id, person_data in list(tracked_persons.items()):\n",
        "            if len(unmatched_detections) == 0:\n",
        "                person_data[\"last_seen\"] = i - 1\n",
        "                continue\n",
        "\n",
        "            best_detection_idx = None\n",
        "            best_similarity = -float('inf')\n",
        "\n",
        "            # Use multiple recent templates for matching\n",
        "            person_embeddings = person_data[\"embeddings\"]\n",
        "\n",
        "            for det_idx in unmatched_detections:\n",
        "                detection = current_detections[det_idx]\n",
        "\n",
        "                # Compute average similarity with templates\n",
        "                similarities = []\n",
        "                for template_embedding in person_embeddings:\n",
        "                    similarity = F.cosine_similarity(\n",
        "                        detection[\"embedding\"].unsqueeze(0),\n",
        "                        template_embedding.unsqueeze(0)\n",
        "                    ).item()\n",
        "                    similarities.append(similarity)\n",
        "\n",
        "                avg_similarity = sum(similarities) / len(similarities)\n",
        "\n",
        "                if avg_similarity > best_similarity:\n",
        "                    best_similarity = avg_similarity\n",
        "                    best_detection_idx = det_idx\n",
        "\n",
        "            # Match if similarity exceeds threshold\n",
        "            if best_similarity > SIMILARITY_THRESHOLD and best_detection_idx is not None:\n",
        "                matched_tracks[person_id] = {\n",
        "                    \"detection_idx\": best_detection_idx,\n",
        "                    \"similarity\": best_similarity\n",
        "                }\n",
        "                unmatched_detections.remove(best_detection_idx)\n",
        "\n",
        "        # Update matched tracks\n",
        "        for person_id, match_info in matched_tracks.items():\n",
        "            det_idx = match_info[\"detection_idx\"]\n",
        "            detection = current_detections[det_idx]\n",
        "\n",
        "            person_data = tracked_persons[person_id]\n",
        "\n",
        "            # 更新 embedding 历史\n",
        "            person_data[\"embeddings\"].append(detection[\"embedding\"])\n",
        "            if len(person_data[\"embeddings\"]) > MAX_EMBEDDING_HISTORY:\n",
        "                person_data[\"embeddings\"].pop(0)\n",
        "\n",
        "            # 更新时间 & 位置 & 置信度\n",
        "            person_data[\"last_seen\"] = i\n",
        "            person_data[\"current_box\"] = detection[\"box\"]\n",
        "            person_data[\"score\"] = detection.get(\"score\", 1.0)\n",
        "\n",
        "        # Create new tracks for unmatched detections\n",
        "        for det_idx in unmatched_detections:\n",
        "            detection = current_detections[det_idx]\n",
        "            tracked_persons[next_id] = {\n",
        "                \"embeddings\": [detection[\"embedding\"]],\n",
        "                \"color\": (randint(0, 255), randint(0, 255), randint(0, 255)),\n",
        "                \"last_seen\": i,\n",
        "                \"current_box\": detection[\"box\"],\n",
        "                \"score\": detection.get(\"score\", 1.0),\n",
        "            }\n",
        "            next_id += 1\n",
        "\n",
        "        # Remove stale tracks\n",
        "        for person_id in list(tracked_persons.keys()):\n",
        "            if i - tracked_persons[person_id][\"last_seen\"] > MAX_FRAMES_MISSING:\n",
        "                del tracked_persons[person_id]\n",
        "\n",
        "        # Draw tracking results for the current frame\n",
        "        annotations = []\n",
        "        for person_id, person_data in tracked_persons.items():\n",
        "            if person_data[\"last_seen\"] == i:\n",
        "                box = person_data[\"current_box\"]\n",
        "                x1, y1, x2, y2 = box.tolist()\n",
        "                w = x2 - x1\n",
        "                h = y2 - y1\n",
        "                conf = float(person_data.get(\"score\", 1.0))\n",
        "\n",
        "                # 写入一行: frame,id,x,y,w,h,conf\n",
        "                # 如果你朋友的 YOLO 文件 frame 是从 1 开始，这里用 i+1 比较方便对齐\n",
        "                frame_idx_for_txt = i + 1\n",
        "                output_file.write(\n",
        "                    f\"{frame_idx_for_txt},{person_id},{x1},{y1},{w},{h},{conf:.4f},1,1,1\\n\"\n",
        "                )\n",
        "\n",
        "                annotations.append((box, person_data[\"color\"], str(person_id)))\n",
        "\n",
        "        # 画框\n",
        "        if annotations:\n",
        "            all_boxes = torch.stack([ann[0] for ann in annotations])\n",
        "            all_colors = [ann[1] for ann in annotations]\n",
        "            all_labels = [ann[2] for ann in annotations]\n",
        "\n",
        "            annotated_img = draw_bounding_boxes_with_labels(\n",
        "                img,\n",
        "                all_boxes.cpu(),\n",
        "                labels=all_labels,\n",
        "                colors=all_colors,\n",
        "                width=4,\n",
        "            )\n",
        "        else:\n",
        "            annotated_img = img\n",
        "\n",
        "        # Save result\n",
        "        write_jpeg(torch.mul(annotated_img, 255.0).to(torch.uint8), f'./out/{i:05d}.jpg')\n",
        "\n",
        "    # 关闭 txt 文件\n",
        "    output_file.close()\n",
        "    print(f\"Tracking results saved to: {output_txt_path}\")\n",
        "\n",
        "    # Visualize\n",
        "    visualize_sequence(sequence_dir, output_txt_path, video_path)\n",
        "\n",
        "    # Metrics\n",
        "    if os.path.exists(gt_txt_path):\n",
        "        evaluate_mot_sequence(gt_txt_path, output_txt_path, csv_path, max_iou=0.5)\n",
        "    else:\n",
        "        print(\"No ground truth, testing sequence\")\n",
        "\n",
        "\n",
        "# Utility function: Enhanced bounding box drawing with labels\n",
        "def draw_bounding_boxes_with_labels(image, boxes, labels=None, colors=None, width=1):\n",
        "    \"\"\"\n",
        "    Draw bounding boxes with labels\n",
        "\n",
        "    Args:\n",
        "        image: Input image tensor [C, H, W]\n",
        "        boxes: Bounding box coordinates [N, 4] in (x1, y1, x2, y2) format\n",
        "        labels: List of labels [N]\n",
        "        colors: List of colors [N]\n",
        "        width: Line width\n",
        "\n",
        "    Returns:\n",
        "        Image with bounding boxes and labels\n",
        "    \"\"\"\n",
        "    image_with_boxes = draw_bounding_boxes(image, boxes, colors=colors, width=width)\n",
        "\n",
        "    if labels is None:\n",
        "        return image_with_boxes\n",
        "\n",
        "    image_pil = transforms.ToPILImage()(image_with_boxes)\n",
        "    draw = ImageDraw.Draw(image_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "        color = colors[i] if colors is not None else (255, 255, 255)\n",
        "        text = labels[i]\n",
        "\n",
        "        if hasattr(draw, 'textbbox'):\n",
        "            bbox = draw.textbbox((0, 0), text, font=font)\n",
        "            text_w = bbox[2] - bbox[0]\n",
        "            text_h = bbox[3] - bbox[1]\n",
        "        else:\n",
        "            text_w, text_h = draw.textsize(text, font=font)\n",
        "\n",
        "        draw.rectangle([(x1, y1 - text_h - 2), (x1 + text_w, y1)], fill=color)\n",
        "        draw.text((x1, y1 - text_h - 2), text, fill=(0, 0, 0), font=font)\n",
        "\n",
        "    return transforms.ToTensor()(image_pil)\n",
        "\n",
        "# Hypothetical extension of Siamese network to add get_embedding()\n",
        "class Siamese_Network_Extended(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(Siamese_Network_Extended, self).__init__()\n",
        "        self.original_model = original_model\n",
        "        self.encoder = original_model.encoder if hasattr(original_model, 'encoder') else None\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        return self.original_model(x1, x2)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        \"\"\"Extract embedding for a single image\"\"\"\n",
        "        if self.encoder:\n",
        "            return self.encoder(x)\n",
        "        else:\n",
        "            dummy = torch.zeros_like(x)\n",
        "            out_x, _ = self.original_model(x, dummy)\n",
        "            return out_x"
      ],
      "metadata": {
        "id": "72tC_Zgb2uNt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main part"
      ],
      "metadata": {
        "id": "x6JllydT6eDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU2qv8QIy2Ch",
        "outputId": "34e6f8fc-9255-4a72-f3b9-2a4da892997a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing frames: 100%|██████████| 1500/1500 [42:54<00:00,  1.72s/frame]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking results saved to: /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/FasterRCNN_tracker_results.txt\n",
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/tracked_test_video.mp4\n",
            "No ground truth, testing sequence\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Entry point to execute the main function\n",
        "import os\n",
        "os.makedirs(f\"{PROJECT_PATH}/out/\", exist_ok=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}