{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 794,
          "status": "ok",
          "timestamp": 1765480689624,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "U0Rxv4MSzw4O",
        "outputId": "22bc965f-2e25-4205-84ff-6c479989ce30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1765480692499,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "BqOUSvxUjZKZ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Deep_Learning/Final Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1765480693779,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "dkZFKe53hF8Z"
      },
      "outputs": [],
      "source": [
        "# Define the root directory path for the project in Google Drive\n",
        "# Update this path to match your own Google Drive directory structure\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Deep_Learning/Final Project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8678,
          "status": "ok",
          "timestamp": 1765480703510,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "b9CkzYpjjfTH",
        "outputId": "f551e24f-9eea-45b5-bff6-823b07e4825e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipynb in /usr/local/lib/python3.12/dist-packages (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install motmetrics -q\n",
        "!pip install ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1765480707256,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "5bgM02EVlpHt",
        "outputId": "2622a08a-6301-47e8-c437-7b8f7d3ec7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Deep_Learning/Final Project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Deep_Learning/Final Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "executionInfo": {
          "elapsed": 43,
          "status": "ok",
          "timestamp": 1765480708504,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "QkN7AnlOyJ84"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for deep learning, computer vision, and tracking\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import configparser\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import motmetrics as mm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from random import randint\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image, write_jpeg, write_video\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Compatibility patch for NumPy 2.x: motmetrics library requires np.asfarray\n",
        "# This function was deprecated in NumPy 2.0, so we provide a fallback implementation\n",
        "if not hasattr(np, \"asfarray\"):\n",
        "    np.asfarray = lambda a: np.asarray(a, dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1765480710559,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "1I3siQUe13Bs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Siamese Network for person re-identification and similarity computation.\n",
        "This network takes pairs of images and generates embeddings that can be compared\n",
        "using cosine similarity to determine if they represent the same person.\n",
        "Input: RGB images of size 3x128x64 (CxHxW)\n",
        "\"\"\"\n",
        "class Siamese_Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Siamese_Network, self).__init__()\n",
        "        # Convolutional layers: progressively increase channels while reducing spatial dimensions\n",
        "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3)\n",
        "        self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=3)\n",
        "        self.conv3 = torch.nn.Conv2d(128, 128, kernel_size=3)\n",
        "        # Fully connected layers: map flattened features to 256-dimensional embedding\n",
        "        self.fc1 = torch.nn.Linear(128*14*6, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        \"\"\"Process a single image through the network to generate an embedding\"\"\"\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "        x = torch.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Process a pair of images and return their embeddings\"\"\"\n",
        "        return self.forward_one(x1), self.forward_one(x2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "executionInfo": {
          "elapsed": 9,
          "status": "ok",
          "timestamp": 1765480712498,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "MoArviAo17Z5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Gallery Dataset for Siamese Network Training\n",
        "Creates positive pairs (same person) and negative pairs (different persons) from a gallery of images.\n",
        "Images are expected to be named with format: {person_id}_{other_info}.jpg\n",
        "\"\"\"\n",
        "class Gallery(torch.utils.data.Dataset):\n",
        "    errorCount = 0\n",
        "    \n",
        "    def __init__(self, path, transform=None, max_pairs_per_id=50, max_neg_pairs_per_id=50):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: Directory containing person images\n",
        "            transform: Optional image transformations\n",
        "            max_pairs_per_id: Maximum positive pairs to generate per person ID\n",
        "            max_neg_pairs_per_id: Maximum negative pairs to generate per person ID\n",
        "        \"\"\"\n",
        "        self.path = path\n",
        "        self.imgs = sorted([x for x in os.listdir(path) if x.endswith('.jpg')])\n",
        "        self.transform = transform\n",
        "        self.max_pairs_per_id = max_pairs_per_id\n",
        "        self.max_neg_pairs_per_id = max_neg_pairs_per_id\n",
        "        self.pairs = []\n",
        "        self.labels = []\n",
        "        self._create_pairs()\n",
        "\n",
        "    def _create_pairs(self):\n",
        "        \"\"\"Generate positive and negative image pairs for training\"\"\"\n",
        "        # Group images by person ID (extracted from filename prefix)\n",
        "        person_images = {}\n",
        "        for img_name in self.imgs:\n",
        "            person_id = img_name.split('_')[0]\n",
        "            if person_id not in person_images:\n",
        "                person_images[person_id] = []\n",
        "            person_images[person_id].append(img_name)\n",
        "\n",
        "        # Create pairs for each person\n",
        "        for person_id, images in person_images.items():\n",
        "            # Positive pairs: all combinations of images from the same person\n",
        "            positive_pairs = [(images[i], images[j]) for i in range(len(images)) for j in range(i + 1, len(images))]\n",
        "            # Sample a subset to limit dataset size\n",
        "            positive_pairs = random.sample(positive_pairs, min(len(positive_pairs), self.max_pairs_per_id))\n",
        "            self.pairs.extend(positive_pairs)\n",
        "            self.labels.extend([1] * len(positive_pairs))\n",
        "\n",
        "            # Negative pairs: images from different persons\n",
        "            other_person_ids = list(person_images.keys())\n",
        "            other_person_ids.remove(person_id)\n",
        "            negative_pairs = []\n",
        "            for other_id in random.sample(other_person_ids, min(len(other_person_ids), self.max_neg_pairs_per_id)):\n",
        "                negative_pairs.append((images[0], person_images[other_id][0]))\n",
        "            self.pairs.extend(negative_pairs)\n",
        "            self.labels.extend([0] * len(negative_pairs))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return a pair of images with their label (1=same person, 0=different)\"\"\"\n",
        "        img1_name, img2_name = self.pairs[idx]\n",
        "        img1_path = os.path.join(self.path, img1_name)\n",
        "        img2_path = os.path.join(self.path, img2_name)\n",
        "\n",
        "        try:\n",
        "            # Load images and normalize to [0, 1] range\n",
        "            img1 = read_image(img1_path).float() / 255.0\n",
        "            img2 = read_image(img2_path).float() / 255.0\n",
        "        except (RuntimeError, OSError) as e:\n",
        "            # Handle corrupted or missing images with placeholder zeros\n",
        "            Gallery.errorCount += 1\n",
        "            print(f\"Error#: {Gallery.errorCount}. Error loading {img1_path} or {img2_path}: {e}\")\n",
        "            img1 = torch.zeros(3, 128, 64)\n",
        "            img2 = torch.zeros(3, 128, 64)\n",
        "\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "        return img1, img2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1765480715808,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "QrKjOsTt2GC4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MOT16 Training Dataset\n",
        "Loads MOT16 training sequences with ground truth annotations for object detection fine-tuning.\n",
        "Each sequence contains images and corresponding bounding boxes with person IDs.\n",
        "\"\"\"\n",
        "class MOT16TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root: Root directory containing MOT16 training sequences (MOT16-02, MOT16-04, etc.)\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.imgs = []\n",
        "        self.targets = []\n",
        "        \n",
        "        # Process each sequence subdirectory\n",
        "        for subdir in os.listdir(root):\n",
        "            # Collect all frame images from the sequence\n",
        "            next_dir = list(sorted(os.listdir(os.path.join(root, subdir, \"img1\"))))\n",
        "            next_dir = [os.path.join(subdir, \"img1\", filename) for filename in next_dir]\n",
        "            self.imgs += next_dir\n",
        "\n",
        "            # Load ground truth annotations (format: frame, id, x, y, w, h, conf, ...)\n",
        "            gt = np.genfromtxt(os.path.join(root, subdir, \"gt\", \"gt.txt\"), delimiter=',', dtype=int)\n",
        "\n",
        "            # Filter to only include person detections (class ID = 1)\n",
        "            person_mask = (gt[:, PERSON_COL] == 1)\n",
        "            gt = gt[person_mask, :]\n",
        "\n",
        "            # Convert MOT16 format (x, y, w, h) to PyTorch format (x1, y1, x2, y2)\n",
        "            # Note: MOT16 'top' column is actually bottom y-coordinate (y-axis is inverted)\n",
        "            bots = gt[:, TOP_COL]\n",
        "            lefts = gt[:, LEFT_COL]\n",
        "            tops = bots + gt[:, HEIGHT_COL]  # Calculate top from bottom + height\n",
        "            rights = lefts + gt[:, WIDTH_COL]  # Calculate right from left + width\n",
        "            boxes = np.column_stack((lefts, bots, rights, tops))\n",
        "            person_ids = gt[:, ID_COL]\n",
        "\n",
        "            # Create target dictionaries for each frame\n",
        "            for i in range(len(next_dir)):\n",
        "                d = {}\n",
        "                # Get all detections for this frame (frames are 1-indexed in MOT16)\n",
        "                mask = (gt[:, FRAME_COL] == i+1)\n",
        "                req_boxes = boxes[mask, :]\n",
        "                req_person_ids = person_ids[mask]\n",
        "                d['boxes'] = torch.tensor(req_boxes, dtype=torch.float)\n",
        "                d['labels'] = torch.ones(mask.shape, dtype=torch.int64)  # All are person class\n",
        "                d['person_ids'] = torch.tensor(req_person_ids, dtype=torch.int64)\n",
        "                self.targets.append(d)\n",
        "\n",
        "        print('Length of dataset: ', len(self.imgs))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load image and corresponding annotations for a given index\"\"\"\n",
        "        if(idx > len(self.imgs)):\n",
        "            return None, None\n",
        "\n",
        "        img_path = os.path.join(self.root, self.imgs[idx])\n",
        "        # Normalize image to [0, 1] range\n",
        "        img = torch.div(read_image(img_path).float(), 255.0)\n",
        "\n",
        "        return img, self.targets[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1765480718240,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "IHtC1h0u2Mb_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "MOT16 Test Dataset\n",
        "Loads MOT16 test sequence images without ground truth annotations.\n",
        "Used for inference and tracking evaluation.\n",
        "\"\"\"\n",
        "class MOT16TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root: Directory containing a single MOT16 test sequence (e.g., MOT16-03)\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"img1\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load and return a single test image\"\"\"\n",
        "        if(idx > len(self.imgs)):\n",
        "            return None, None\n",
        "\n",
        "        img_path = os.path.join(self.root, \"img1\", self.imgs[idx])\n",
        "        # Normalize image to [0, 1] range\n",
        "        img = torch.div(read_image(img_path).float(), 255.0)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1765480720527,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "wiztbc5S2QVe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load and configure Faster R-CNN detector model for person detection.\n",
        "The model is initialized with ImageNet pretrained weights and fine-tuned for person detection.\n",
        "\"\"\"\n",
        "def get_detector_model(load_weights=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        load_weights: If True, load fine-tuned weights from model_param_path\n",
        "    \n",
        "    Returns:\n",
        "        Configured Faster R-CNN model ready for inference or training\n",
        "    \"\"\"\n",
        "    # Initialize with ImageNet pretrained weights\n",
        "    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "    \n",
        "    # Replace the classification head to match our number of classes (background + person)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
        "    \n",
        "    # Load fine-tuned weights if available\n",
        "    if load_weights:\n",
        "        if os.path.exists(model_param_path):\n",
        "            model.load_state_dict(torch.load(model_param_path, weights_only=True))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX41hlx3zvGH"
      },
      "outputs": [],
      "source": [
        "!unzip -q f\"{PROJECT_PATH}/MOT16.zip\" -d /content/MOT16_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1765480722653,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "7V9VPDOtyuOr"
      },
      "outputs": [],
      "source": [
        "# Configuration: Define file paths and tracking parameters\n",
        "# Model paths\n",
        "similarity_model_path = f'{PROJECT_PATH}/siamese_network.pth'  # Siamese network for person re-identification\n",
        "model_param_path = f'{PROJECT_PATH}/models_FasterRCNN/bbox_detector.pth'  # Faster R-CNN detector weights\n",
        "\n",
        "# Data paths\n",
        "sequence_dir = f'{PROJECT_PATH}/MOT16/test/MOT16-03'  # MOT16 test sequence directory\n",
        "gt_txt_path = os.path.join(sequence_dir, \"gt/gt.txt\")  # Ground truth annotations (if available)\n",
        "\n",
        "# Output paths\n",
        "output_txt_path = f'{PROJECT_PATH}/results_FasterRCNN/FasterRCNN_tracker_results.txt'  # Tracking results in MOT format\n",
        "video_path = f'{PROJECT_PATH}/results_FasterRCNN/tracked_test_video.mp4'  # Visualized tracking video\n",
        "csv_path = f'{PROJECT_PATH}/results_FasterRCNN/FasterRCNN_tracker_metrics.csv'  # Evaluation metrics\n",
        "\n",
        "# Detection and tracking parameters\n",
        "BBOX_SCORE_THRESH = 0.7  # Minimum confidence threshold for bounding box detections\n",
        "NUM_CLASSES = 2  # Number of classes: background (0) and person (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1765480724477,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "SKUpW-fJyu8-"
      },
      "outputs": [],
      "source": [
        "# Visualization Functions: Generate annotated video from tracking results\n",
        "\n",
        "def get_sequence_info(sequence_dir):\n",
        "    \"\"\"\n",
        "    Extract sequence metadata (FPS, resolution) from seqinfo.ini file.\n",
        "    \n",
        "    Args:\n",
        "        sequence_dir: Path to MOT16 sequence directory\n",
        "    \n",
        "    Returns:\n",
        "        fps: Frames per second\n",
        "        (width, height): Video resolution tuple\n",
        "    \"\"\"\n",
        "    seqinfo_path = os.path.join(sequence_dir, \"seqinfo.ini\")\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(seqinfo_path)\n",
        "    fps = None\n",
        "    width = None\n",
        "    height = None\n",
        "    fps = config.getint(\"Sequence\", \"frameRate\", fallback=fps)\n",
        "    width = config.getint(\"Sequence\", \"imWidth\", fallback=width)\n",
        "    height = config.getint(\"Sequence\", \"imHeight\", fallback=height)\n",
        "    return fps, (width, height)\n",
        "\n",
        "def visualize_sequence(sequence_dir, results_file, output_video_path):\n",
        "    \"\"\"\n",
        "    Create an annotated video showing tracked persons with colored bounding boxes and IDs.\n",
        "    Each tracked person is assigned a unique color for visual distinction.\n",
        "    \n",
        "    Args:\n",
        "        sequence_dir: Path to MOT16 sequence directory containing images\n",
        "        results_file: Path to tracking results file (MOT format: frame,id,x,y,w,h,conf,...)\n",
        "        output_video_path: Path where the output video will be saved\n",
        "    \"\"\"\n",
        "    # Load tracking results from text file\n",
        "    results_df = pd.read_csv(results_file, header=None)\n",
        "    results_df.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"x3\",\"y3\",\"z3\"]\n",
        "\n",
        "    # Get all frame images from the sequence\n",
        "    image_paths = sorted(glob.glob(os.path.join(sequence_dir, \"img1\", \"*.jpg\")))\n",
        "\n",
        "    # Initialize video writer with sequence metadata\n",
        "    fps, frame_size = get_sequence_info(sequence_dir)\n",
        "    first_img = cv2.imread(image_paths[0])\n",
        "    frame_size = (first_img.shape[1], first_img.shape[0])  # Use actual image dimensions\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
        "\n",
        "    # Generate consistent color palette for each track ID (seed for reproducibility)\n",
        "    max_id = int(results_df[\"id\"].max()) + 1\n",
        "    np.random.seed(42)  # Fixed seed ensures same colors across runs\n",
        "    colors = np.random.randint(0, 255, size=(max_id, 3), dtype=np.uint8)\n",
        "\n",
        "    # Annotate each frame with bounding boxes and track IDs\n",
        "    for img_path in image_paths:\n",
        "        frame_num = int(os.path.splitext(os.path.basename(img_path))[0])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Get all detections for this frame\n",
        "        frame_data = results_df[results_df[\"frame\"] == frame_num]\n",
        "        for _, row in frame_data.iterrows():\n",
        "            track_id = int(row[\"id\"])\n",
        "            color = tuple(map(int, colors[track_id]))\n",
        "            x, y, w_box, h_box = int(row[\"x\"]), int(row[\"y\"]), int(row[\"w\"]), int(row[\"h\"])\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(img, (x, y), (x + w_box, y + h_box), color, 2)\n",
        "            # Draw track ID label above the box\n",
        "            cv2.putText(img, str(track_id), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "        video_writer.write(img)\n",
        "    \n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {output_video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "executionInfo": {
          "elapsed": 18,
          "status": "ok",
          "timestamp": 1765480729228,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "3q7y8pGFUOZq"
      },
      "outputs": [],
      "source": [
        "# Evaluation Metrics: Compute MOTChallenge standard metrics for tracking performance\n",
        "\n",
        "def evaluate_mot_sequence(gt_txt_path, output_txt_path, csv_path, max_iou=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate tracking performance using MOTChallenge standard metrics.\n",
        "    Computes metrics such as MOTA, MOTP, IDF1, and other tracking statistics.\n",
        "    \n",
        "    Args:\n",
        "        gt_txt_path: Path to ground truth tracking file (MOT format)\n",
        "        output_txt_path: Path to tracker output file (MOT format)\n",
        "        csv_path: Path to save evaluation metrics CSV\n",
        "        max_iou: Maximum IoU threshold for considering a detection as a match (default: 0.5)\n",
        "    \n",
        "    Returns:\n",
        "        Metrics summary including MOTA, MOTP, IDF1, etc.\n",
        "    \"\"\"\n",
        "    # Load ground truth and tracker output files\n",
        "    gt = pd.read_csv(gt_txt_path, header=None)\n",
        "    tr = pd.read_csv(output_txt_path, header=None)\n",
        "\n",
        "    # Extract standard MOT format columns: frame, id, x, y, w, h, conf\n",
        "    gt = gt.iloc[:, :7]\n",
        "    tr = tr.iloc[:, :7]\n",
        "\n",
        "    gt.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
        "    tr.columns = [\"frame\",\"id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
        "\n",
        "    # Filter to only include valid detections (confidence > 0)\n",
        "    # In MOT format, conf > 0 indicates valid evaluation boxes\n",
        "    gt = gt[gt[\"conf\"] > 0].copy()\n",
        "    tr = tr[tr[\"conf\"] > 0].copy()\n",
        "\n",
        "    # Initialize MOTAccumulator for computing tracking metrics\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "\n",
        "    # Process each frame to compute detection-to-track associations\n",
        "    frames = sorted(gt[\"frame\"].unique())\n",
        "    for f in frames:\n",
        "        gt_frame = gt[gt[\"frame\"]==f]\n",
        "        tr_frame = tr[tr[\"frame\"]==f]\n",
        "\n",
        "        gt_ids = gt_frame[\"id\"].tolist()\n",
        "        tr_ids = tr_frame[\"id\"].tolist()\n",
        "\n",
        "        gt_boxes = gt_frame[[\"x\",\"y\",\"w\",\"h\"]].values\n",
        "        tr_boxes = tr_frame[[\"x\",\"y\",\"w\",\"h\"]].values\n",
        "\n",
        "        # Compute IoU-based distance matrix between ground truth and tracker detections\n",
        "        # Distance = 1 - IoU, so lower distance means better match\n",
        "        distances = 1 - mm.distances.iou_matrix(gt_boxes, tr_boxes, max_iou=max_iou)\n",
        "        acc.update(gt_ids, tr_ids, distances)\n",
        "\n",
        "    # Compute MOTChallenge standard metrics\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(acc, metrics=mm.metrics.motchallenge_metrics)\n",
        "    summary.to_csv(csv_path)\n",
        "    print(f\"[OK] Metrics saved to {csv_path}\")\n",
        "    print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1765480732581,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "dE211kuyyyf2"
      },
      "outputs": [],
      "source": [
        "# Image Preprocessing: Resize cropped person images to fixed size for Siamese network\n",
        "\n",
        "def resize_image(image):\n",
        "    \"\"\"\n",
        "    Resize an image to the standard input size for the Siamese network using adaptive pooling.\n",
        "    This ensures all person crops are normalized to the same dimensions (128x64) regardless\n",
        "    of their original size, which is required for the network architecture.\n",
        "    \n",
        "    Args:\n",
        "        image: Input image tensor of shape [C, H, W] (typically a cropped person bounding box)\n",
        "    \n",
        "    Returns:\n",
        "        Resized image tensor of shape [C, 128, 64]\n",
        "    \"\"\"\n",
        "    return F.adaptive_avg_pool2d(image, (128, 64))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "executionInfo": {
          "elapsed": 29,
          "status": "ok",
          "timestamp": 1765480797637,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "72tC_Zgb2uNt"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main tracking pipeline: Multi-object tracking using Faster R-CNN detection and Siamese network re-identification.\n",
        "    \n",
        "    The pipeline processes video frames sequentially:\n",
        "    1. Detect persons in each frame using Faster R-CNN\n",
        "    2. Extract embeddings for each detection using Siamese network\n",
        "    3. Associate detections to existing tracks using cosine similarity\n",
        "    4. Update tracks or create new ones for unmatched detections\n",
        "    5. Remove stale tracks that haven't been seen recently\n",
        "    6. Save results in MOT format and generate visualization\n",
        "    \"\"\"\n",
        "    # Initialize device (GPU if available, else CPU)\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    bbox_detector = get_detector_model(load_weights=False)\n",
        "    bbox_detector.load_state_dict(torch.load(model_param_path))\n",
        "    bbox_detector.to(device)\n",
        "    bbox_detector.eval()\n",
        "\n",
        "    similarity_model = Siamese_Network().to(device)\n",
        "    similarity_model.load_state_dict(torch.load(similarity_model_path, weights_only=True))\n",
        "    similarity_model.eval()\n",
        "    similarity_model = Siamese_Network_Extended(similarity_model)\n",
        "\n",
        "    img_set = MOT16TestDataset(sequence_dir)\n",
        "\n",
        "    # Tracking state management\n",
        "    next_id = 0\n",
        "    tracked_persons = {}  # {id: {\"embeddings\": [last N frame embeddings], \"color\": (R,G,B), \"last_seen\": frame_idx}}\n",
        "    MAX_EMBEDDING_HISTORY = 5  # Keep embeddings from last N frames\n",
        "    MAX_FRAMES_MISSING = 30    # Max frames allowed missing\n",
        "    SIMILARITY_THRESHOLD = 0.6 # Threshold below which it is considered a new target\n",
        "\n",
        "\n",
        "    # Open output file for writing tracking results in MOT format\n",
        "    output_file = open(output_txt_path, \"w\")\n",
        "\n",
        "    # Process each frame in the sequence with progress bar\n",
        "    for i, img in tqdm(enumerate(img_set), total=len(img_set), desc=\"Processing frames\", unit=\"frame\"):\n",
        "        img_device = img.to(device)\n",
        "\n",
        "        # Step 1: Object Detection - Detect all persons in the current frame\n",
        "        bbox_pred = bbox_detector([img_device])[0]\n",
        "        scores = bbox_pred[\"scores\"]\n",
        "        boxes = bbox_pred[\"boxes\"]\n",
        "        # Filter detections by confidence threshold to remove low-confidence detections\n",
        "        mask = (scores[:] >= BBOX_SCORE_THRESH)\n",
        "        boxes = boxes[mask].type(torch.int32)\n",
        "        scores = scores[mask]\n",
        "\n",
        "        # Step 2: Extract embeddings for each detected person\n",
        "        # For each detection, crop the person region, resize it, and compute embedding\n",
        "        current_detections = []\n",
        "        for box, score in zip(boxes, scores):\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            # Validate bounding box dimensions (must have positive width and height)\n",
        "            if x2 - x1 > 0 and y2 - y1 > 0:\n",
        "                # Crop person region and resize to standard size (128x64)\n",
        "                cropped = resize_image(img_device[:, y1:y2, x1:x2]).to(device)\n",
        "                # Generate embedding using Siamese network for person re-identification\n",
        "                embedding = similarity_model.get_embedding(cropped)\n",
        "                current_detections.append(\n",
        "                    {\n",
        "                        \"box\": box,\n",
        "                        \"embedding\": embedding,\n",
        "                        \"score\": float(score.item()),  # Detection confidence score\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # Step 3: Data Association - Match detections to existing tracks\n",
        "        # Initialize: all detections start as unmatched\n",
        "        unmatched_detections = list(range(len(current_detections)))\n",
        "        matched_tracks = {}\n",
        "\n",
        "        # For each existing track, find the best matching detection\n",
        "        for person_id, person_data in list(tracked_persons.items()):\n",
        "            if len(unmatched_detections) == 0:\n",
        "                # No detections left to match, mark track as not seen in this frame\n",
        "                person_data[\"last_seen\"] = i - 1\n",
        "                continue\n",
        "\n",
        "            best_detection_idx = None\n",
        "            best_similarity = -float('inf')\n",
        "\n",
        "            # Use multiple recent embeddings (templates) for robust matching\n",
        "            # This helps handle appearance changes over time\n",
        "            person_embeddings = person_data[\"embeddings\"]\n",
        "\n",
        "            # Evaluate similarity between track and each unmatched detection\n",
        "            for det_idx in unmatched_detections:\n",
        "                detection = current_detections[det_idx]\n",
        "\n",
        "                # Compute cosine similarity with each historical embedding\n",
        "                similarities = []\n",
        "                for template_embedding in person_embeddings:\n",
        "                    similarity = F.cosine_similarity(\n",
        "                        detection[\"embedding\"].unsqueeze(0),\n",
        "                        template_embedding.unsqueeze(0)\n",
        "                    ).item()\n",
        "                    similarities.append(similarity)\n",
        "\n",
        "                # Use average similarity across all templates for more stable matching\n",
        "                avg_similarity = sum(similarities) / len(similarities)\n",
        "\n",
        "                # Track the best match\n",
        "                if avg_similarity > best_similarity:\n",
        "                    best_similarity = avg_similarity\n",
        "                    best_detection_idx = det_idx\n",
        "\n",
        "            # Associate detection to track if similarity exceeds threshold\n",
        "            if best_similarity > SIMILARITY_THRESHOLD and best_detection_idx is not None:\n",
        "                matched_tracks[person_id] = {\n",
        "                    \"detection_idx\": best_detection_idx,\n",
        "                    \"similarity\": best_similarity\n",
        "                }\n",
        "                unmatched_detections.remove(best_detection_idx)\n",
        "\n",
        "        # Step 4: Update matched tracks with new detections\n",
        "        for person_id, match_info in matched_tracks.items():\n",
        "            det_idx = match_info[\"detection_idx\"]\n",
        "            detection = current_detections[det_idx]\n",
        "\n",
        "            person_data = tracked_persons[person_id]\n",
        "\n",
        "            # Update embedding history: add new embedding, maintain sliding window\n",
        "            person_data[\"embeddings\"].append(detection[\"embedding\"])\n",
        "            if len(person_data[\"embeddings\"]) > MAX_EMBEDDING_HISTORY:\n",
        "                person_data[\"embeddings\"].pop(0)  # Remove oldest embedding\n",
        "\n",
        "            # Update track state: current frame, position, and detection confidence\n",
        "            person_data[\"last_seen\"] = i\n",
        "            person_data[\"current_box\"] = detection[\"box\"]\n",
        "            person_data[\"score\"] = detection.get(\"score\", 1.0)\n",
        "\n",
        "        # Step 5: Initialize new tracks for unmatched detections\n",
        "        # Unmatched detections are assumed to be new persons entering the scene\n",
        "        for det_idx in unmatched_detections:\n",
        "            detection = current_detections[det_idx]\n",
        "            tracked_persons[next_id] = {\n",
        "                \"embeddings\": [detection[\"embedding\"]],  # Initialize with first embedding\n",
        "                \"color\": (randint(0, 255), randint(0, 255), randint(0, 255)),  # Random color for visualization\n",
        "                \"last_seen\": i,\n",
        "                \"current_box\": detection[\"box\"],\n",
        "                \"score\": detection.get(\"score\", 1.0),\n",
        "            }\n",
        "            next_id += 1\n",
        "\n",
        "        # Step 6: Remove stale tracks that haven't been seen for too long\n",
        "        # This handles cases where persons leave the scene or are occluded\n",
        "        for person_id in list(tracked_persons.keys()):\n",
        "            if i - tracked_persons[person_id][\"last_seen\"] > MAX_FRAMES_MISSING:\n",
        "                del tracked_persons[person_id]\n",
        "\n",
        "        # Step 7: Save tracking results and visualize current frame\n",
        "        annotations = []\n",
        "        for person_id, person_data in tracked_persons.items():\n",
        "            # Only process tracks that were seen in the current frame\n",
        "            if person_data[\"last_seen\"] == i:\n",
        "                box = person_data[\"current_box\"]\n",
        "                x1, y1, x2, y2 = box.tolist()\n",
        "                w = x2 - x1\n",
        "                h = y2 - y1\n",
        "                conf = float(person_data.get(\"score\", 1.0))\n",
        "\n",
        "                # Write tracking result in MOT format: frame,id,x,y,w,h,conf,x3,y3,z3\n",
        "                # Frame numbers are 1-indexed in MOT format (i+1)\n",
        "                frame_idx_for_txt = i + 1\n",
        "                output_file.write(\n",
        "                    f\"{frame_idx_for_txt},{person_id},{x1},{y1},{w},{h},{conf:.4f},1,1,1\\n\"\n",
        "                )\n",
        "\n",
        "                # Collect annotations for visualization\n",
        "                annotations.append((box, person_data[\"color\"], str(person_id)))\n",
        "\n",
        "        # Draw bounding boxes and track IDs on the frame\n",
        "        if annotations:\n",
        "            all_boxes = torch.stack([ann[0] for ann in annotations])\n",
        "            all_colors = [ann[1] for ann in annotations]\n",
        "            all_labels = [ann[2] for ann in annotations]\n",
        "\n",
        "            annotated_img = draw_bounding_boxes_with_labels(\n",
        "                img,\n",
        "                all_boxes.cpu(),\n",
        "                labels=all_labels,\n",
        "                colors=all_colors,\n",
        "                width=4,\n",
        "            )\n",
        "        else:\n",
        "            annotated_img = img\n",
        "\n",
        "        # Save annotated frame image\n",
        "        write_jpeg(torch.mul(annotated_img, 255.0).to(torch.uint8), f'./out/{i:05d}.jpg')\n",
        "\n",
        "    # Finalize: Close output file and generate results\n",
        "    output_file.close()\n",
        "    print(f\"Tracking results saved to: {output_txt_path}\")\n",
        "\n",
        "    # Generate visualization video with tracked bounding boxes\n",
        "    visualize_sequence(sequence_dir, output_txt_path, video_path)\n",
        "\n",
        "    # Compute evaluation metrics if ground truth is available\n",
        "    if os.path.exists(gt_txt_path):\n",
        "        evaluate_mot_sequence(gt_txt_path, output_txt_path, csv_path, max_iou=0.5)\n",
        "    else:\n",
        "        print(\"No ground truth available - this is a test sequence without annotations\")\n",
        "\n",
        "\n",
        "# Visualization Utility: Draw bounding boxes with text labels on images\n",
        "\n",
        "def draw_bounding_boxes_with_labels(image, boxes, labels=None, colors=None, width=1):\n",
        "    \"\"\"\n",
        "    Enhanced bounding box drawing function that adds text labels above each box.\n",
        "    This extends torchvision's draw_bounding_boxes to include track ID labels.\n",
        "    \n",
        "    Args:\n",
        "        image: Input image tensor [C, H, W] in range [0, 1]\n",
        "        boxes: Bounding box coordinates [N, 4] in (x1, y1, x2, y2) format\n",
        "        labels: List of text labels [N] (typically track IDs)\n",
        "        colors: List of RGB color tuples [N] for each bounding box\n",
        "        width: Line width for bounding box borders\n",
        "    \n",
        "    Returns:\n",
        "        Image tensor with bounding boxes and labels drawn\n",
        "    \"\"\"\n",
        "    image_with_boxes = draw_bounding_boxes(image, boxes, colors=colors, width=width)\n",
        "\n",
        "    if labels is None:\n",
        "        return image_with_boxes\n",
        "\n",
        "    image_pil = transforms.ToPILImage()(image_with_boxes)\n",
        "    draw = ImageDraw.Draw(image_pil)\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "        color = colors[i] if colors is not None else (255, 255, 255)\n",
        "        text = labels[i]\n",
        "\n",
        "        if hasattr(draw, 'textbbox'):\n",
        "            bbox = draw.textbbox((0, 0), text, font=font)\n",
        "            text_w = bbox[2] - bbox[0]\n",
        "            text_h = bbox[3] - bbox[1]\n",
        "        else:\n",
        "            text_w, text_h = draw.textsize(text, font=font)\n",
        "\n",
        "        draw.rectangle([(x1, y1 - text_h - 2), (x1 + text_w, y1)], fill=color)\n",
        "        draw.text((x1, y1 - text_h - 2), text, fill=(0, 0, 0), font=font)\n",
        "\n",
        "    return transforms.ToTensor()(image_pil)\n",
        "\n",
        "# Hypothetical extension of Siamese network to add get_embedding()\n",
        "# Siamese Network Extension: Wrapper to enable single-image embedding extraction\n",
        "# The original Siamese network requires pairs of images, but for tracking we need\n",
        "# to extract embeddings from individual detections. This wrapper enables that functionality.\n",
        "\n",
        "class Siamese_Network_Extended(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper around Siamese_Network to enable embedding extraction from single images.\n",
        "    The original network expects pairs, but for tracking we need single-image embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, original_model):\n",
        "        super(Siamese_Network_Extended, self).__init__()\n",
        "        self.original_model = original_model\n",
        "        # Check if model has a direct encoder (for potential optimization)\n",
        "        self.encoder = original_model.encoder if hasattr(original_model, 'encoder') else None\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Standard forward pass with image pairs\"\"\"\n",
        "        return self.original_model(x1, x2)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        \"\"\"\n",
        "        Extract embedding for a single image.\n",
        "        Since the Siamese network expects pairs, we use a dummy second image.\n",
        "        \n",
        "        Args:\n",
        "            x: Single image tensor [C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            Embedding vector for the input image\n",
        "        \"\"\"\n",
        "        if self.encoder:\n",
        "            return self.encoder(x)\n",
        "        else:\n",
        "            # Use dummy second image to satisfy network's pair requirement\n",
        "            dummy = torch.zeros_like(x)\n",
        "            out_x, _ = self.original_model(x, dummy)\n",
        "            return out_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6JllydT6eDt"
      },
      "source": [
        "# Main part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2643892,
          "status": "ok",
          "timestamp": 1765483443902,
          "user": {
            "displayName": "Joseph Marinello",
            "userId": "09666882728420153032"
          },
          "user_tz": 360
        },
        "id": "zU2qv8QIy2Ch",
        "outputId": "34e6f8fc-9255-4a72-f3b9-2a4da892997a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing frames: 100%|██████████| 1500/1500 [42:54<00:00,  1.72s/frame]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tracking results saved to: /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/FasterRCNN_tracker_results.txt\n",
            "Video saved to /content/drive/MyDrive/Deep_Learning/Final Project/results_FasterRCNN/tracked_test_video.mp4\n",
            "No ground truth, testing sequence\n"
          ]
        }
      ],
      "source": [
        "# Main Execution: Run the complete tracking pipeline\n",
        "# This cell executes the full tracking workflow: detection, association, and evaluation\n",
        "\n",
        "import os\n",
        "# Create output directory for annotated frame images\n",
        "os.makedirs(f\"{PROJECT_PATH}/out/\", exist_ok=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [
        {
          "file_id": "1-fLknYR4EYTc7uHo5nyP6aCfu7zKoaFC",
          "timestamp": 1763082095545
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
